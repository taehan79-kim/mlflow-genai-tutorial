{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“˜ Step 1: MLflow Tracking - ì‹¤í—˜ ì¶”ì  ê¸°ì´ˆ\n",
    "\n",
    "## ëª©í‘œ\n",
    "- MLflow Trackingì˜ ê¸°ë³¸ ê°œë… ì´í•´\n",
    "- Parameters, Metrics, Artifacts ë¡œê¹…\n",
    "- MLflow UIì—ì„œ ì‹¤í—˜ ê²°ê³¼ í™•ì¸\n",
    "\n",
    "## í•™ìŠµ í¬ì¸íŠ¸\n",
    "- `mlflow.start_run()` ì‚¬ìš©ë²•\n",
    "- `log_param()`, `log_metric()`, `log_artifact()` ì°¨ì´\n",
    "- Run, Experiment ê°œë…\n",
    "- ì—¬ëŸ¬ ì‹¤í—˜ ë¹„êµ ë° ë¶„ì„"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. ì‚¬ì „ ì¤€ë¹„\n",
    "\n",
    "### âš ï¸ ì£¼ì˜ì‚¬í•­\n",
    "ì´ ë…¸íŠ¸ë¶ì„ ì‹¤í–‰í•˜ê¸° ì „ì— **Step 0**ì„ ë¨¼ì € ì™„ë£Œí•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "- `00_setup_baseline_rag_agent.ipynb` ì‹¤í–‰ ì™„ë£Œ\n",
    "- `.env` íŒŒì¼ ì„¤ì • ì™„ë£Œ\n",
    "- í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ ì„¤ì • ë° íŒ¨í‚¤ì§€ ì„í¬íŠ¸\n",
    "\n",
    "### 1.1 í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ\n",
      "MLflow ë²„ì „: 3.7.0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "from typing import TypedDict, List\n",
    "from datetime import datetime\n",
    "\n",
    "# MLflow\n",
    "import mlflow\n",
    "from mlflow import MlflowClient\n",
    "\n",
    "# LangChain & LangGraph\n",
    "from langchain_aws import ChatBedrock, BedrockEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "print(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ\")\n",
    "print(f\"MLflow ë²„ì „: {mlflow.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MLflow Tracking ì„¤ì •\n",
    "\n",
    "### 2.1 Tracking URI ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… MLflow Tracking URI ì„¤ì • ì™„ë£Œ\n",
      "ğŸ“ Tracking URI: ./mlruns\n"
     ]
    }
   ],
   "source": [
    "# ë¡œì»¬ ë””ë ‰í† ë¦¬ì— MLflow ë°ì´í„° ì €ì¥\n",
    "mlflow.set_tracking_uri(\"./mlruns\")\n",
    "\n",
    "print(\"âœ… MLflow Tracking URI ì„¤ì • ì™„ë£Œ\")\n",
    "print(f\"ğŸ“ Tracking URI: {mlflow.get_tracking_uri()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Experiment ìƒì„±\n",
    "\n",
    "**Experiment**: ê´€ë ¨ëœ ì—¬ëŸ¬ Runì„ ê·¸ë£¹í™”í•˜ëŠ” ë‹¨ìœ„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hank_kim/Documents/taehan/mlflow-genai-tutorial/.venv/lib/python3.13/site-packages/mlflow/tracking/_tracking_service/utils.py:177: FutureWarning: The filesystem tracking backend (e.g., './mlruns') will be deprecated in February 2026. Consider transitioning to a database backend (e.g., 'sqlite:///mlflow.db') to take advantage of the latest MLflow features. See https://github.com/mlflow/mlflow/issues/18534 for more details and migration guidance.\n",
      "  return FileStore(store_uri, store_uri)\n",
      "2025/12/13 00:29:19 INFO mlflow.tracking.fluent: Experiment with name 'rag_agent_experiments' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Experiment ì„¤ì • ì™„ë£Œ\n",
      "ğŸ“Š Experiment Name: rag_agent_experiments\n",
      "ğŸ†” Experiment ID: 313401264484788163\n",
      "ğŸ“ Artifact Location: /Users/hank_kim/Documents/taehan/mlflow-genai-tutorial/mlruns/313401264484788163\n"
     ]
    }
   ],
   "source": [
    "# Experiment ì´ë¦„ ì„¤ì •\n",
    "experiment_name = \"rag_agent_experiments\"\n",
    "\n",
    "# Experiment ìƒì„± (ì´ë¯¸ ìˆìœ¼ë©´ ê¸°ì¡´ ê²ƒ ì‚¬ìš©)\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# Experiment ì •ë³´ í™•ì¸\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "\n",
    "print(\"âœ… Experiment ì„¤ì • ì™„ë£Œ\")\n",
    "print(f\"ğŸ“Š Experiment Name: {experiment.name}\")\n",
    "print(f\"ğŸ†” Experiment ID: {experiment.experiment_id}\")\n",
    "print(f\"ğŸ“ Artifact Location: {experiment.artifact_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RAG Agent ì¬êµ¬ì„±\n",
    "\n",
    "Step 0ì—ì„œ ë§Œë“  RAG Agentë¥¼ ë‹¤ì‹œ êµ¬ì„±í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 ìƒ˜í”Œ ë¬¸ì„œ ë° Vector Store ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 5ê°œì˜ ìƒ˜í”Œ ë¬¸ì„œ ì¤€ë¹„ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# ìƒ˜í”Œ ë¬¸ì„œ (Step 0ê³¼ ë™ì¼)\n",
    "sample_documents = [\n",
    "    \"\"\"\n",
    "    RAG (Retrieval-Augmented Generation)ëŠ” ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.\n",
    "    RAGëŠ” ì™¸ë¶€ ì§€ì‹ ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ê³ , ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë” ì •í™•í•˜ê³  ì‚¬ì‹¤ì— ê¸°ë°˜í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "    ì´ ë°©ë²•ì€ ëª¨ë¸ì´ í•™ìŠµí•˜ì§€ ì•Šì€ ìµœì‹  ì •ë³´ë‚˜ íŠ¹ì • ë„ë©”ì¸ ì§€ì‹ì„ í™œìš©í•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Vector StoreëŠ” ë¬¸ì„œì˜ ì„ë² ë”© ë²¡í„°ë¥¼ ì €ì¥í•˜ê³  ê²€ìƒ‰í•˜ëŠ” ë°ì´í„°ë² ì´ìŠ¤ì…ë‹ˆë‹¤.\n",
    "    FAISS, Chroma, Pinecone ë“±ì´ ëŒ€í‘œì ì¸ Vector Storeì…ë‹ˆë‹¤.\n",
    "    Vector StoreëŠ” ìœ ì‚¬ë„ ê²€ìƒ‰(Similarity Search)ì„ í†µí•´ ì¿¼ë¦¬ì™€ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œë¥¼ ë¹ ë¥´ê²Œ ì°¾ì•„ëƒ…ë‹ˆë‹¤.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    LangGraphëŠ” LangChain ìœ„ì— êµ¬ì¶•ëœ ìƒíƒœ ê¸°ë°˜ ê·¸ë˜í”„ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.\n",
    "    ë³µì¡í•œ AI ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš°ë¥¼ ëª…í™•í•œ ìƒíƒœì™€ ë…¸ë“œë¡œ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "    ê° ë…¸ë“œëŠ” íŠ¹ì • ì‘ì—…ì„ ìˆ˜í–‰í•˜ê³ , ì—£ì§€ëŠ” ë…¸ë“œ ê°„ì˜ ì „í™˜ ì¡°ê±´ì„ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    EmbeddingsëŠ” í…ìŠ¤íŠ¸ë¥¼ ê³ ì°¨ì› ë²¡í„° ê³µê°„ì˜ ì ìœ¼ë¡œ í‘œí˜„í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.\n",
    "    OpenAIì˜ text-embedding-3-small, text-embedding-3-large ëª¨ë¸ì´ ë„ë¦¬ ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
    "    ì¢‹ì€ ì„ë² ë”©ì€ ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„° ê³µê°„ì—ì„œ ê°€ê¹Œì´ ìœ„ì¹˜ì‹œí‚µë‹ˆë‹¤.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Chunkingì€ ê¸´ ë¬¸ì„œë¥¼ ì‘ì€ ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.\n",
    "    ì ì ˆí•œ chunk í¬ê¸°ëŠ” ê²€ìƒ‰ ì •í™•ë„ì™€ ì»¨í…ìŠ¤íŠ¸ ì–‘ì˜ ê· í˜•ì„ ë§ì¶”ëŠ” ë° ì¤‘ìš”í•©ë‹ˆë‹¤.\n",
    "    ì¼ë°˜ì ìœ¼ë¡œ 512~1024 í† í° í¬ê¸°ì˜ chunkê°€ ë§ì´ ì‚¬ìš©ë˜ë©°, ë¬¸ì¥ ê²½ê³„ë¥¼ ê³ ë ¤í•œ ë¶„í• ì´ íš¨ê³¼ì ì…ë‹ˆë‹¤.\n",
    "    \"\"\",\n",
    "]\n",
    "\n",
    "print(f\"âœ… {len(sample_documents)}ê°œì˜ ìƒ˜í”Œ ë¬¸ì„œ ì¤€ë¹„ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… create_vector_store í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "def create_vector_store(chunk_size: int = 512, chunk_overlap: int = 50):\n",
    "    \"\"\"\n",
    "    Vector Storeë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜\n",
    "    \n",
    "    Args:\n",
    "        chunk_size: í…ìŠ¤íŠ¸ë¥¼ ë‚˜ëˆŒ í¬ê¸°\n",
    "        chunk_overlap: chunk ê°„ ì¤‘ë³µ í¬ê¸°\n",
    "    \"\"\"\n",
    "    # í…ìŠ¤íŠ¸ ë¶„í• ê¸°\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "    )\n",
    "    \n",
    "    # ë¬¸ì„œ ë¶„í• \n",
    "    documents = [Document(page_content=doc.strip()) for doc in sample_documents]\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    \n",
    "    # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”\n",
    "    embeddings = BedrockEmbeddings(\n",
    "        model_id=os.environ[\"AWS_EMD_MODEL_ID\"],\n",
    "        aws_access_key_id=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "        aws_secret_access_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "        region_name=os.environ[\"AWS_REGION\"],\n",
    "    )\n",
    "    \n",
    "    # Vector Store ìƒì„±\n",
    "    vector_store = FAISS.from_documents(split_docs, embeddings)\n",
    "    \n",
    "    return vector_store, len(split_docs)\n",
    "\n",
    "print(\"âœ… create_vector_store í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 RAG State ë° Nodes ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… RAGState ì •ì˜ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "class RAGState(TypedDict):\n",
    "    \"\"\"RAG Agentì˜ ìƒíƒœ\"\"\"\n",
    "    query: str\n",
    "    retrieved_documents: List[Document]\n",
    "    context: str\n",
    "    answer: str\n",
    "    metadata: dict\n",
    "\n",
    "print(\"âœ… RAGState ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… create_rag_agent í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "def create_rag_agent(vector_store, top_k: int = 3, llm_model: str = \"gpt-4o-mini\", temperature: float = 0.7):\n",
    "    \"\"\"\n",
    "    RAG Agentë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜\n",
    "    \n",
    "    Args:\n",
    "        vector_store: FAISS vector store\n",
    "        top_k: ê²€ìƒ‰í•  ë¬¸ì„œ ê°œìˆ˜\n",
    "        llm_model: ì‚¬ìš©í•  LLM ëª¨ë¸\n",
    "        temperature: LLM temperature\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retriever Node\n",
    "    def retriever_node(state: RAGState) -> RAGState:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        retrieved_docs = vector_store.similarity_search(state[\"query\"], k=top_k)\n",
    "        \n",
    "        state[\"retrieved_documents\"] = retrieved_docs\n",
    "        state[\"metadata\"][\"retrieval_time\"] = time.time() - start_time\n",
    "        state[\"metadata\"][\"num_retrieved_docs\"] = len(retrieved_docs)\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    # Generator Node\n",
    "    def generator_node(state: RAGState) -> RAGState:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Context ìƒì„±\n",
    "        context = \"\\n\\n\".join([\n",
    "            f\"[ë¬¸ì„œ {i+1}]\\n{doc.page_content}\"\n",
    "            for i, doc in enumerate(state[\"retrieved_documents\"])\n",
    "        ])\n",
    "        state[\"context\"] = context\n",
    "        \n",
    "        # Prompt í…œí”Œë¦¿\n",
    "        prompt_template = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"ë‹¹ì‹ ì€ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê³  ê°„ê²°í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.\"),\n",
    "            (\"human\", \"ì»¨í…ìŠ¤íŠ¸:\\n{context}\\n\\nì§ˆë¬¸: {query}\\n\\në‹µë³€:\")\n",
    "        ])\n",
    "        \n",
    "        # LLM ì´ˆê¸°í™”\n",
    "        llm = ChatBedrock(\n",
    "            model_id=os.environ[\"AWS_MODEL_ID\"],\n",
    "            aws_access_key_id=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "            aws_secret_access_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "            region_name=os.environ[\"AWS_REGION\"],\n",
    "            temperature=0.0,\n",
    "        )\n",
    "        \n",
    "        # ë‹µë³€ ìƒì„±\n",
    "        messages = prompt_template.format_messages(context=context, query=state[\"query\"])\n",
    "        response = llm.invoke(messages)\n",
    "        \n",
    "        state[\"answer\"] = response.content\n",
    "        state[\"metadata\"][\"generation_time\"] = time.time() - start_time\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    # Graph êµ¬ì„±\n",
    "    workflow = StateGraph(RAGState)\n",
    "    workflow.add_node(\"retriever\", retriever_node)\n",
    "    workflow.add_node(\"generator\", generator_node)\n",
    "    workflow.add_edge(START, \"retriever\")\n",
    "    workflow.add_edge(\"retriever\", \"generator\")\n",
    "    workflow.add_edge(\"generator\", END)\n",
    "    \n",
    "    return workflow.compile()\n",
    "\n",
    "print(\"âœ… create_rag_agent í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. MLflowë¡œ ì²« ë²ˆì§¸ Run ê¸°ë¡í•˜ê¸°\n",
    "\n",
    "### 4.1 ê¸°ë³¸ Run ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸš€ ì²« ë²ˆì§¸ MLflow Run ì‹œì‘\n",
      "================================================================================\n",
      "âœ… Parameters ë¡œê¹… ì™„ë£Œ\n",
      "âœ… Vector Store ìƒì„± ì™„ë£Œ (ì´ 5ê°œ chunks)\n",
      "\n",
      "âœ… RAG Agent ì‹¤í–‰ ì™„ë£Œ\n",
      "ğŸ’¬ ë‹µë³€: RAG(Retrieval-Augmented Generation)ëŠ” ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.\n",
      "\n",
      "**ì£¼ìš” íŠ¹ì§•:**\n",
      "- ì™¸ë¶€ ì§€ì‹ ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ì •...\n",
      "âœ… Metrics ë¡œê¹… ì™„ë£Œ\n",
      "âœ… Artifacts ë¡œê¹… ì™„ë£Œ\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Š Run ì •ë³´\n",
      "================================================================================\n",
      "ğŸ†” Run ID: d8bdf68c0679464ea9f09fdbba644e08\n",
      "ğŸ“Š Experiment ID: 313401264484788163\n",
      "ğŸ“ Artifact URI: /Users/hank_kim/Documents/taehan/mlflow-genai-tutorial/mlruns/313401264484788163/d8bdf68c0679464ea9f09fdbba644e08/artifacts\n",
      "â±ï¸  ì‹¤í–‰ ì‹œê°„: 5.057ì´ˆ\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ğŸš€ ì²« ë²ˆì§¸ MLflow Run ì‹œì‘\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# MLflow Run ì‹œì‘\n",
    "with mlflow.start_run(run_name=\"baseline_rag_v1\") as run:\n",
    "    \n",
    "    # 1. Parameters ë¡œê¹…\n",
    "    chunk_size = 512\n",
    "    chunk_overlap = 50\n",
    "    top_k = 3\n",
    "    llm_model = os.environ[\"AWS_MODEL_ID\"]\n",
    "    temperature = 0.0\n",
    "    emd_model = os.environ[\"AWS_EMD_MODEL_ID\"]\n",
    "    \n",
    "    mlflow.log_param(\"chunk_size\", chunk_size)\n",
    "    mlflow.log_param(\"chunk_overlap\", chunk_overlap)\n",
    "    mlflow.log_param(\"top_k\", top_k)\n",
    "    mlflow.log_param(\"llm_model\", llm_model)\n",
    "    mlflow.log_param(\"temperature\", temperature)\n",
    "    mlflow.log_param(\"embedding_model\", emd_model)\n",
    "    \n",
    "    print(\"âœ… Parameters ë¡œê¹… ì™„ë£Œ\")\n",
    "    \n",
    "    # 2. Vector Store ë° RAG Agent ìƒì„±\n",
    "    vector_store, num_chunks = create_vector_store(chunk_size, chunk_overlap)\n",
    "    rag_agent = create_rag_agent(vector_store, top_k, llm_model, temperature)\n",
    "    \n",
    "    mlflow.log_param(\"num_chunks\", num_chunks)\n",
    "    print(f\"âœ… Vector Store ìƒì„± ì™„ë£Œ (ì´ {num_chunks}ê°œ chunks)\")\n",
    "    \n",
    "    # 3. í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì‹¤í–‰\n",
    "    test_query = \"RAGê°€ ë¬´ì—‡ì¸ê°€ìš”?\"\n",
    "    \n",
    "    initial_state = {\n",
    "        \"query\": test_query,\n",
    "        \"retrieved_documents\": [],\n",
    "        \"context\": \"\",\n",
    "        \"answer\": \"\",\n",
    "        \"metadata\": {}\n",
    "    }\n",
    "    \n",
    "    overall_start = time.time()\n",
    "    result = rag_agent.invoke(initial_state)\n",
    "    overall_time = time.time() - overall_start\n",
    "    \n",
    "    print(f\"\\nâœ… RAG Agent ì‹¤í–‰ ì™„ë£Œ\")\n",
    "    print(f\"ğŸ’¬ ë‹µë³€: {result['answer'][:100]}...\")\n",
    "    \n",
    "    # 4. Metrics ë¡œê¹…\n",
    "    mlflow.log_metric(\"overall_time\", overall_time)\n",
    "    mlflow.log_metric(\"retrieval_time\", result['metadata']['retrieval_time'])\n",
    "    mlflow.log_metric(\"generation_time\", result['metadata']['generation_time'])\n",
    "    mlflow.log_metric(\"num_retrieved_docs\", result['metadata']['num_retrieved_docs'])\n",
    "    mlflow.log_metric(\"answer_length\", len(result['answer']))\n",
    "    \n",
    "    print(\"âœ… Metrics ë¡œê¹… ì™„ë£Œ\")\n",
    "    \n",
    "    # 5. Artifacts ë¡œê¹…\n",
    "    # 5.1 ë‹µë³€ì„ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥\n",
    "    mlflow.log_text(result['answer'], \"output_answer.txt\")\n",
    "    \n",
    "    # 5.2 ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ JSONìœ¼ë¡œ ì €ì¥\n",
    "    retrieved_docs_data = [\n",
    "        {\"index\": i, \"content\": doc.page_content}\n",
    "        for i, doc in enumerate(result['retrieved_documents'])\n",
    "    ]\n",
    "    mlflow.log_dict(retrieved_docs_data, \"retrieved_documents.json\")\n",
    "    \n",
    "    # 5.3 ì‹¤í–‰ ì •ë³´ë¥¼ JSONìœ¼ë¡œ ì €ì¥\n",
    "    run_info = {\n",
    "        \"query\": test_query,\n",
    "        \"answer\": result['answer'],\n",
    "        \"metrics\": result['metadata'],\n",
    "        \"parameters\": {\n",
    "            \"chunk_size\": chunk_size,\n",
    "            \"top_k\": top_k,\n",
    "            \"llm_model\": llm_model\n",
    "        }\n",
    "    }\n",
    "    mlflow.log_dict(run_info, \"run_summary.json\")\n",
    "    \n",
    "    print(\"âœ… Artifacts ë¡œê¹… ì™„ë£Œ\")\n",
    "    \n",
    "    # Run ì •ë³´ ì¶œë ¥\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"ğŸ“Š Run ì •ë³´\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"ğŸ†” Run ID: {run.info.run_id}\")\n",
    "    print(f\"ğŸ“Š Experiment ID: {run.info.experiment_id}\")\n",
    "    print(f\"ğŸ“ Artifact URI: {run.info.artifact_uri}\")\n",
    "    print(f\"â±ï¸  ì‹¤í–‰ ì‹œê°„: {overall_time:.3f}ì´ˆ\")\n",
    "    print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ì—¬ëŸ¬ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°í•© ì‹¤í—˜\n",
    "\n",
    "ë‹¤ì–‘í•œ íŒŒë¼ë¯¸í„° ì¡°í•©ìœ¼ë¡œ ì—¬ëŸ¬ Runì„ ìƒì„±í•˜ì—¬ ë¹„êµí•©ë‹ˆë‹¤.\n",
    "\n",
    "### 5.1 ì‹¤í—˜ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 5ê°œì˜ ì‹¤í—˜ ì„¤ì • ì¤€ë¹„ ì™„ë£Œ\n",
      "âœ… 2ê°œì˜ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì¤€ë¹„ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# ì‹¤í—˜í•  íŒŒë¼ë¯¸í„° ì¡°í•©\n",
    "experiment_configs = [\n",
    "    {\"name\": \"small_chunks_low_k\", \"chunk_size\": 256, \"top_k\": 3, \"temperature\": 0.7},\n",
    "    {\"name\": \"medium_chunks_medium_k\", \"chunk_size\": 512, \"top_k\": 5, \"temperature\": 0.7},\n",
    "    {\"name\": \"large_chunks_high_k\", \"chunk_size\": 1024, \"top_k\": 10, \"temperature\": 0.7},\n",
    "    {\"name\": \"medium_chunks_low_temp\", \"chunk_size\": 512, \"top_k\": 3, \"temperature\": 0.3},\n",
    "    {\"name\": \"medium_chunks_high_temp\", \"chunk_size\": 512, \"top_k\": 3, \"temperature\": 1.0},\n",
    "]\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸í•  ì¿¼ë¦¬ë“¤\n",
    "test_queries = [\n",
    "    \"RAGê°€ ë¬´ì—‡ì¸ê°€ìš”?\",\n",
    "    \"Vector Storeì˜ ì—­í• ì€ ë¬´ì—‡ì¸ê°€ìš”?\",\n",
    "]\n",
    "\n",
    "print(f\"âœ… {len(experiment_configs)}ê°œì˜ ì‹¤í—˜ ì„¤ì • ì¤€ë¹„ ì™„ë£Œ\")\n",
    "print(f\"âœ… {len(test_queries)}ê°œì˜ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì¤€ë¹„ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 ë°°ì¹˜ ì‹¤í—˜ ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ”¬ ë°°ì¹˜ ì‹¤í—˜ ì‹œì‘\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "ì‹¤í—˜ 1/5: small_chunks_low_k\n",
      "================================================================================\n",
      "âœ… ì™„ë£Œ - í‰ê·  ì‹¤í–‰ ì‹œê°„: 4.935ì´ˆ\n",
      "ğŸ†” Run ID: 98b1875988ee4fd4b47151de5b6d550c\n",
      "\n",
      "================================================================================\n",
      "ì‹¤í—˜ 2/5: medium_chunks_medium_k\n",
      "================================================================================\n",
      "âœ… ì™„ë£Œ - í‰ê·  ì‹¤í–‰ ì‹œê°„: 5.811ì´ˆ\n",
      "ğŸ†” Run ID: 071788b87b1249c3adda7d45e2b3d4df\n",
      "\n",
      "================================================================================\n",
      "ì‹¤í—˜ 3/5: large_chunks_high_k\n",
      "================================================================================\n",
      "âœ… ì™„ë£Œ - í‰ê·  ì‹¤í–‰ ì‹œê°„: 5.908ì´ˆ\n",
      "ğŸ†” Run ID: 2e145713bb344f3098c3d54b62798521\n",
      "\n",
      "================================================================================\n",
      "ì‹¤í—˜ 4/5: medium_chunks_low_temp\n",
      "================================================================================\n",
      "âœ… ì™„ë£Œ - í‰ê·  ì‹¤í–‰ ì‹œê°„: 5.101ì´ˆ\n",
      "ğŸ†” Run ID: 6babc51e59a740b9af29c9ac309178a3\n",
      "\n",
      "================================================================================\n",
      "ì‹¤í—˜ 5/5: medium_chunks_high_temp\n",
      "================================================================================\n",
      "âœ… ì™„ë£Œ - í‰ê·  ì‹¤í–‰ ì‹œê°„: 4.860ì´ˆ\n",
      "ğŸ†” Run ID: 84ef7495b61a44d49cbcc90965479969\n",
      "\n",
      "================================================================================\n",
      "âœ… ì´ 5ê°œì˜ ì‹¤í—˜ ì™„ë£Œ\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ”¬ ë°°ì¹˜ ì‹¤í—˜ ì‹œì‘\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "all_run_ids = []\n",
    "\n",
    "for idx, config in enumerate(experiment_configs, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ì‹¤í—˜ {idx}/{len(experiment_configs)}: {config['name']}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    with mlflow.start_run(run_name=config['name']) as run:\n",
    "        llm_model = os.environ[\"AWS_MODEL_ID\"]\n",
    "        emd_model = os.environ[\"AWS_EMD_MODEL_ID\"]\n",
    "        # Parameters ë¡œê¹…\n",
    "        mlflow.log_param(\"chunk_size\", config['chunk_size'])\n",
    "        mlflow.log_param(\"chunk_overlap\", 50)\n",
    "        mlflow.log_param(\"top_k\", config['top_k'])\n",
    "        mlflow.log_param(\"llm_model\", llm_model)\n",
    "        mlflow.log_param(\"temperature\", config['temperature'])\n",
    "        mlflow.log_param(\"embedding_model\", emd_model)\n",
    "        \n",
    "        # Vector Store ë° RAG Agent ìƒì„±\n",
    "        vector_store, num_chunks = create_vector_store(\n",
    "            chunk_size=config['chunk_size'],\n",
    "            chunk_overlap=50\n",
    "        )\n",
    "        mlflow.log_param(\"num_chunks\", num_chunks)\n",
    "        \n",
    "        rag_agent = create_rag_agent(\n",
    "            vector_store=vector_store,\n",
    "            top_k=config['top_k'],\n",
    "            llm_model=\"gpt-4o-mini\",\n",
    "            temperature=config['temperature']\n",
    "        )\n",
    "        \n",
    "        # ì—¬ëŸ¬ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸\n",
    "        total_time = 0\n",
    "        total_retrieval_time = 0\n",
    "        total_generation_time = 0\n",
    "        \n",
    "        for query_idx, query in enumerate(test_queries):\n",
    "            initial_state = {\n",
    "                \"query\": query,\n",
    "                \"retrieved_documents\": [],\n",
    "                \"context\": \"\",\n",
    "                \"answer\": \"\",\n",
    "                \"metadata\": {}\n",
    "            }\n",
    "            \n",
    "            start_time = time.time()\n",
    "            result = rag_agent.invoke(initial_state)\n",
    "            elapsed_time = time.time() - start_time\n",
    "            \n",
    "            total_time += elapsed_time\n",
    "            total_retrieval_time += result['metadata']['retrieval_time']\n",
    "            total_generation_time += result['metadata']['generation_time']\n",
    "            \n",
    "            # ì¿¼ë¦¬ë³„ ë©”íŠ¸ë¦­ ë¡œê¹…\n",
    "            mlflow.log_metric(f\"query_{query_idx}_time\", elapsed_time)\n",
    "            mlflow.log_metric(f\"query_{query_idx}_answer_length\", len(result['answer']))\n",
    "        \n",
    "        # í‰ê·  ë©”íŠ¸ë¦­ ê³„ì‚° ë° ë¡œê¹…\n",
    "        num_queries = len(test_queries)\n",
    "        mlflow.log_metric(\"avg_overall_time\", total_time / num_queries)\n",
    "        mlflow.log_metric(\"avg_retrieval_time\", total_retrieval_time / num_queries)\n",
    "        mlflow.log_metric(\"avg_generation_time\", total_generation_time / num_queries)\n",
    "        mlflow.log_metric(\"total_time\", total_time)\n",
    "        \n",
    "        # Run ID ì €ì¥\n",
    "        all_run_ids.append(run.info.run_id)\n",
    "        \n",
    "        print(f\"âœ… ì™„ë£Œ - í‰ê·  ì‹¤í–‰ ì‹œê°„: {total_time / num_queries:.3f}ì´ˆ\")\n",
    "        print(f\"ğŸ†” Run ID: {run.info.run_id}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"âœ… ì´ {len(all_run_ids)}ê°œì˜ ì‹¤í—˜ ì™„ë£Œ\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. MLflow UIì—ì„œ ê²°ê³¼ í™•ì¸\n",
    "\n",
    "### 6.1 MLflow UI ì‹¤í–‰ ë°©ë²•\n",
    "\n",
    "í„°ë¯¸ë„ì—ì„œ ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”:\n",
    "\n",
    "```bash\n",
    "mlflow ui --port 5000\n",
    "```\n",
    "\n",
    "ë˜ëŠ” ë‹¤ë¥¸ í¬íŠ¸ ì‚¬ìš©:\n",
    "\n",
    "```bash\n",
    "mlflow ui --port 8080\n",
    "```\n",
    "\n",
    "ê·¸ëŸ° ë‹¤ìŒ ë¸Œë¼ìš°ì €ì—ì„œ `http://localhost:5000` (ë˜ëŠ” ì„¤ì •í•œ í¬íŠ¸)ë¡œ ì ‘ì†í•˜ì„¸ìš”.\n",
    "\n",
    "### 6.2 UIì—ì„œ í™•ì¸í•  ë‚´ìš©\n",
    "\n",
    "1. **Experiments í˜ì´ì§€**\n",
    "   - `rag_agent_experiments` ì‹¤í—˜ ì„ íƒ\n",
    "   - ëª¨ë“  Run ëª©ë¡ í™•ì¸\n",
    "\n",
    "2. **Run ë¹„êµ**\n",
    "   - ì—¬ëŸ¬ Run ì„ íƒ (ì²´í¬ë°•ìŠ¤)\n",
    "   - \"Compare\" ë²„íŠ¼ í´ë¦­\n",
    "   - Parameters, Metrics ë¹„êµ\n",
    "\n",
    "3. **ì°¨íŠ¸ ì‹œê°í™”**\n",
    "   - Parallel Coordinates Plot\n",
    "   - Scatter Plot (Parameters vs Metrics)\n",
    "   - Box Plot\n",
    "\n",
    "4. **ê°œë³„ Run ìƒì„¸ ì •ë³´**\n",
    "   - Parameters íƒ­\n",
    "   - Metrics íƒ­\n",
    "   - Artifacts íƒ­ (ìƒì„±ëœ íŒŒì¼ë“¤)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Python APIë¡œ Run ì •ë³´ ì¡°íšŒí•˜ê¸°\n",
    "\n",
    "### 7.1 MlflowClientë¡œ Run ê²€ìƒ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š ì´ 6ê°œì˜ Run ë°œê²¬\n",
      "\n",
      "================================================================================\n",
      "Run Name                       Avg Time     Top K    Chunk Size  \n",
      "================================================================================\n",
      "medium_chunks_high_temp        4.860        3        512         \n",
      "small_chunks_low_k             4.935        3        256         \n",
      "medium_chunks_low_temp         5.101        3        512         \n",
      "medium_chunks_medium_k         5.811        5        512         \n",
      "large_chunks_high_k            5.908        10       1024        \n",
      "baseline_rag_v1                0.000        3        512         \n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# MlflowClient ì´ˆê¸°í™”\n",
    "client = MlflowClient()\n",
    "\n",
    "# í˜„ì¬ ì‹¤í—˜ì˜ ëª¨ë“  Run ì¡°íšŒ\n",
    "experiment = mlflow.get_experiment_by_name(\"rag_agent_experiments\")\n",
    "runs = client.search_runs(\n",
    "    experiment_ids=[experiment.experiment_id],\n",
    "    order_by=[\"metrics.avg_overall_time ASC\"]  # í‰ê·  ì‹¤í–‰ ì‹œê°„ ê¸°ì¤€ ì •ë ¬\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ“Š ì´ {len(runs)}ê°œì˜ Run ë°œê²¬\\n\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Run Name':<30} {'Avg Time':<12} {'Top K':<8} {'Chunk Size':<12}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for run in runs[:10]:  # ìµœëŒ€ 10ê°œë§Œ ì¶œë ¥\n",
    "    run_name = run.data.tags.get('mlflow.runName', 'N/A')\n",
    "    avg_time = run.data.metrics.get('avg_overall_time', 0)\n",
    "    top_k = run.data.params.get('top_k', 'N/A')\n",
    "    chunk_size = run.data.params.get('chunk_size', 'N/A')\n",
    "    \n",
    "    print(f\"{run_name:<30} {avg_time:<12.3f} {top_k:<8} {chunk_size:<12}\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 ìµœê³  ì„±ëŠ¥ Run ì°¾ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ† ìµœê³  ì„±ëŠ¥ Run\n",
      "================================================================================\n",
      "\n",
      "ğŸ†” Run ID: 84ef7495b61a44d49cbcc90965479969\n",
      "ğŸ“ Run Name: medium_chunks_high_temp\n",
      "\n",
      "ğŸ“Š Parameters:\n",
      "  â€¢ chunk_overlap: 50\n",
      "  â€¢ num_chunks: 5\n",
      "  â€¢ top_k: 3\n",
      "  â€¢ chunk_size: 512\n",
      "  â€¢ embedding_model: amazon.titan-embed-text-v2:0\n",
      "  â€¢ temperature: 1.0\n",
      "  â€¢ llm_model: global.anthropic.claude-sonnet-4-5-20250929-v1:0\n",
      "\n",
      "ğŸ“ˆ Metrics:\n",
      "  â€¢ query_0_time: 4.501ì´ˆ\n",
      "  â€¢ avg_overall_time: 4.860ì´ˆ\n",
      "  â€¢ query_1_time: 5.220ì´ˆ\n",
      "  â€¢ total_time: 9.721ì´ˆ\n",
      "  â€¢ query_0_answer_length: 281.00\n",
      "  â€¢ query_1_answer_length: 315.00\n",
      "  â€¢ avg_retrieval_time: 0.111ì´ˆ\n",
      "  â€¢ avg_generation_time: 4.747ì´ˆ\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ê°€ì¥ ë¹ ë¥¸ ì‹¤í–‰ ì‹œê°„ì„ ê°€ì§„ Run ì°¾ê¸°\n",
    "if runs:\n",
    "    best_run = runs[0]  # ì´ë¯¸ avg_overall_timeìœ¼ë¡œ ì •ë ¬ë¨\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ğŸ† ìµœê³  ì„±ëŠ¥ Run\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nğŸ†” Run ID: {best_run.info.run_id}\")\n",
    "    print(f\"ğŸ“ Run Name: {best_run.data.tags.get('mlflow.runName', 'N/A')}\")\n",
    "    \n",
    "    print(\"\\nğŸ“Š Parameters:\")\n",
    "    for key, value in best_run.data.params.items():\n",
    "        print(f\"  â€¢ {key}: {value}\")\n",
    "    \n",
    "    print(\"\\nğŸ“ˆ Metrics:\")\n",
    "    for key, value in best_run.data.metrics.items():\n",
    "        if 'time' in key:\n",
    "            print(f\"  â€¢ {key}: {value:.3f}ì´ˆ\")\n",
    "        else:\n",
    "            print(f\"  â€¢ {key}: {value:.2f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Runì˜ Artifacts í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ“ Artifacts (Run: medium_chunks_high_temp)\n",
      "================================================================================\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ì²« ë²ˆì§¸ Runì˜ Artifacts ëª©ë¡ ì¡°íšŒ\n",
    "if runs:\n",
    "    sample_run = runs[0]\n",
    "    artifacts = client.list_artifacts(sample_run.info.run_id)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"ğŸ“ Artifacts (Run: {sample_run.data.tags.get('mlflow.runName', 'N/A')})\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for artifact in artifacts:\n",
    "        print(f\"  â€¢ {artifact.path} ({artifact.file_size} bytes)\")\n",
    "    \n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ì‹¤í—˜ ê²°ê³¼ ë¶„ì„ ë° ì‹œê°í™”\n",
    "\n",
    "### 8.1 ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š ì‹¤í—˜ ê²°ê³¼ ìš”ì•½\n",
      "\n",
      "                          run_id                run_name  chunk_size  top_k  temperature  avg_time  avg_retrieval_time  avg_generation_time\n",
      "84ef7495b61a44d49cbcc90965479969 medium_chunks_high_temp         512      3          1.0  4.860455            0.111152             4.747003\n",
      "98b1875988ee4fd4b47151de5b6d550c      small_chunks_low_k         256      3          0.7  4.935054            0.125673             4.806494\n",
      "6babc51e59a740b9af29c9ac309178a3  medium_chunks_low_temp         512      3          0.3  5.100586            0.121225             4.975662\n",
      "071788b87b1249c3adda7d45e2b3d4df  medium_chunks_medium_k         512      5          0.7  5.810857            0.109547             5.695684\n",
      "2e145713bb344f3098c3d54b62798521     large_chunks_high_k        1024     10          0.7  5.907521            0.121863             5.781771\n",
      "d8bdf68c0679464ea9f09fdbba644e08         baseline_rag_v1         512      3          0.0  0.000000            0.000000             0.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Run ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜\n",
    "data = []\n",
    "for run in runs:\n",
    "    row = {\n",
    "        'run_id': run.info.run_id,\n",
    "        'run_name': run.data.tags.get('mlflow.runName', 'N/A'),\n",
    "        'chunk_size': int(run.data.params.get('chunk_size', 0)),\n",
    "        'top_k': int(run.data.params.get('top_k', 0)),\n",
    "        'temperature': float(run.data.params.get('temperature', 0)),\n",
    "        'avg_time': run.data.metrics.get('avg_overall_time', 0),\n",
    "        'avg_retrieval_time': run.data.metrics.get('avg_retrieval_time', 0),\n",
    "        'avg_generation_time': run.data.metrics.get('avg_generation_time', 0),\n",
    "    }\n",
    "    data.append(row)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"\\nğŸ“Š ì‹¤í—˜ ê²°ê³¼ ìš”ì•½\\n\")\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 ê°„ë‹¨í•œ ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ“ˆ ì‹¤í—˜ ê²°ê³¼ ë¶„ì„\n",
      "================================================================================\n",
      "\n",
      "1ï¸âƒ£ Chunk Sizeë³„ í‰ê·  ì‹¤í–‰ ì‹œê°„:\n",
      "  â€¢ 512: 3.943ì´ˆ\n",
      "  â€¢ 256: 4.935ì´ˆ\n",
      "  â€¢ 1024: 5.908ì´ˆ\n",
      "\n",
      "2ï¸âƒ£ Top-Kë³„ í‰ê·  ì‹¤í–‰ ì‹œê°„:\n",
      "  â€¢ 3: 3.724ì´ˆ\n",
      "  â€¢ 5: 5.811ì´ˆ\n",
      "  â€¢ 10: 5.908ì´ˆ\n",
      "\n",
      "3ï¸âƒ£ Temperatureë³„ í‰ê·  ì‹¤í–‰ ì‹œê°„:\n",
      "  â€¢ 0.0: 0.000ì´ˆ\n",
      "  â€¢ 1.0: 4.860ì´ˆ\n",
      "  â€¢ 0.3: 5.101ì´ˆ\n",
      "  â€¢ 0.7: 5.551ì´ˆ\n",
      "\n",
      "4ï¸âƒ£ ì „ì²´ í†µê³„:\n",
      "  â€¢ í‰ê·  ì‹¤í–‰ ì‹œê°„: 4.436ì´ˆ (Â± 2.218)\n",
      "  â€¢ ìµœì†Œ ì‹¤í–‰ ì‹œê°„: 0.000ì´ˆ\n",
      "  â€¢ ìµœëŒ€ ì‹¤í–‰ ì‹œê°„: 5.908ì´ˆ\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“ˆ ì‹¤í—˜ ê²°ê³¼ ë¶„ì„\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# chunk_sizeì— ë”°ë¥¸ í‰ê·  ì‹œê°„\n",
    "print(\"\\n1ï¸âƒ£ Chunk Sizeë³„ í‰ê·  ì‹¤í–‰ ì‹œê°„:\")\n",
    "chunk_analysis = df.groupby('chunk_size')['avg_time'].mean().sort_values()\n",
    "for chunk_size, avg_time in chunk_analysis.items():\n",
    "    print(f\"  â€¢ {chunk_size}: {avg_time:.3f}ì´ˆ\")\n",
    "\n",
    "# top_kì— ë”°ë¥¸ í‰ê·  ì‹œê°„\n",
    "print(\"\\n2ï¸âƒ£ Top-Kë³„ í‰ê·  ì‹¤í–‰ ì‹œê°„:\")\n",
    "topk_analysis = df.groupby('top_k')['avg_time'].mean().sort_values()\n",
    "for top_k, avg_time in topk_analysis.items():\n",
    "    print(f\"  â€¢ {top_k}: {avg_time:.3f}ì´ˆ\")\n",
    "\n",
    "# temperatureì— ë”°ë¥¸ í‰ê·  ì‹œê°„\n",
    "print(\"\\n3ï¸âƒ£ Temperatureë³„ í‰ê·  ì‹¤í–‰ ì‹œê°„:\")\n",
    "temp_analysis = df.groupby('temperature')['avg_time'].mean().sort_values()\n",
    "for temp, avg_time in temp_analysis.items():\n",
    "    print(f\"  â€¢ {temp}: {avg_time:.3f}ì´ˆ\")\n",
    "\n",
    "# ì „ì²´ í†µê³„\n",
    "print(\"\\n4ï¸âƒ£ ì „ì²´ í†µê³„:\")\n",
    "print(f\"  â€¢ í‰ê·  ì‹¤í–‰ ì‹œê°„: {df['avg_time'].mean():.3f}ì´ˆ (Â± {df['avg_time'].std():.3f})\")\n",
    "print(f\"  â€¢ ìµœì†Œ ì‹¤í–‰ ì‹œê°„: {df['avg_time'].min():.3f}ì´ˆ\")\n",
    "print(f\"  â€¢ ìµœëŒ€ ì‹¤í–‰ ì‹œê°„: {df['avg_time'].max():.3f}ì´ˆ\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Runì— íƒœê·¸ ì¶”ê°€í•˜ê¸°\n",
    "\n",
    "Runì— íƒœê·¸ë¥¼ ì¶”ê°€í•˜ì—¬ ë¶„ë¥˜ ë° ê²€ìƒ‰ì„ ì‰½ê²Œ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Run 84ef7495b61a44d49cbcc90965479969ì— íƒœê·¸ ì¶”ê°€ ì™„ë£Œ\n",
      "  â€¢ performance: best\n",
      "  â€¢ status: production_candidate\n"
     ]
    }
   ],
   "source": [
    "# ìµœê³  ì„±ëŠ¥ Runì— íƒœê·¸ ì¶”ê°€\n",
    "if runs:\n",
    "    best_run_id = runs[0].info.run_id\n",
    "    \n",
    "    client.set_tag(best_run_id, \"performance\", \"best\")\n",
    "    client.set_tag(best_run_id, \"status\", \"production_candidate\")\n",
    "    \n",
    "    print(f\"âœ… Run {best_run_id}ì— íƒœê·¸ ì¶”ê°€ ì™„ë£Œ\")\n",
    "    print(\"  â€¢ performance: best\")\n",
    "    print(\"  â€¢ status: production_candidate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. ìš”ì•½ ë° ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "### âœ… ì™„ë£Œí•œ ì‘ì—…\n",
    "1. MLflow Tracking URI ë° Experiment ì„¤ì •\n",
    "2. ì²« ë²ˆì§¸ Run ìƒì„± ë° Parameters, Metrics, Artifacts ë¡œê¹…\n",
    "3. ì—¬ëŸ¬ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°í•©ìœ¼ë¡œ ë°°ì¹˜ ì‹¤í—˜ ì‹¤í–‰\n",
    "4. MLflow UIì—ì„œ ê²°ê³¼ í™•ì¸ ë°©ë²• í•™ìŠµ\n",
    "5. Python APIë¡œ Run ì¡°íšŒ ë° ë¶„ì„\n",
    "\n",
    "### ğŸ“Š ì£¼ìš” í•™ìŠµ ë‚´ìš©\n",
    "- **Run**: ë‹¨ì¼ ì‹¤í—˜ ì‹¤í–‰ (Parameters + Metrics + Artifacts)\n",
    "- **Experiment**: ê´€ë ¨ëœ ì—¬ëŸ¬ Runì˜ ê·¸ë£¹\n",
    "- **Parameters**: ì‹¤í—˜ ì„¤ì •ê°’ (chunk_size, top_k ë“±)\n",
    "- **Metrics**: ì¸¡ì • ê²°ê³¼ (ì‹¤í–‰ ì‹œê°„, ì •í™•ë„ ë“±)\n",
    "- **Artifacts**: ìƒì„±ëœ íŒŒì¼ (ëª¨ë¸, í…ìŠ¤íŠ¸, JSON ë“±)\n",
    "\n",
    "### ğŸ¯ ë‹¤ìŒ ë‹¨ê³„ (Step 2)\n",
    "ë‹¤ìŒ ë…¸íŠ¸ë¶ì—ì„œëŠ” **MLflow Tracing (Autolog)**ì„ í™œìš©í•˜ì—¬:\n",
    "- LangChain Autologë¡œ ì›ë¼ì¸ ìë™ ì¶”ì \n",
    "- Trace êµ¬ì¡° ì´í•´ (Span, Parent-Child)\n",
    "- Jupyter Notebookì—ì„œ ì‹¤ì‹œê°„ Trace ì‹œê°í™”\n",
    "- Token usage ìë™ ì¶”ì \n",
    "\n",
    "â†’ `02_mlflow_tracing_autolog.ipynb`ë¡œ ê³„ì†í•˜ì„¸ìš”!\n",
    "\n",
    "### ğŸ’¡ ì¶”ê°€ ì‹¤ìŠµ ì•„ì´ë””ì–´\n",
    "1. ë‹¤ë¥¸ embedding ëª¨ë¸ ë¹„êµ (text-embedding-3-small vs large)\n",
    "2. ë‹¤ì–‘í•œ LLM ëª¨ë¸ í…ŒìŠ¤íŠ¸ (gpt-4o-mini vs gpt-4o)\n",
    "3. ë” ë§ì€ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ë¡œ í‰ê°€\n",
    "4. ì»¤ìŠ¤í…€ Metrics ì¶”ê°€ (ë‹µë³€ í’ˆì§ˆ, ì •í™•ë„ ë“±)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“š ì°¸ê³  ìë£Œ\n",
    "\n",
    "- [MLflow Tracking Documentation](https://www.mlflow.org/docs/2.21.3/tracking/)\n",
    "- [MLflow Python API - Logging Functions](https://www.mlflow.org/docs/2.21.3/python_api/mlflow.html#mlflow.log_param)\n",
    "- [MLflow Concepts](https://www.mlflow.org/docs/2.21.3/concepts.html)\n",
    "- [MLflow Quickstart](https://www.mlflow.org/docs/2.21.3/getting-started/intro-quickstart/index.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlflow-genai-tutorial (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
