{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“˜ Step 3: MLflow Tracing - ìˆ˜ë™ ì»¤ìŠ¤í…€ Span\n",
    "\n",
    "## ëª©í‘œ\n",
    "- ì»¤ìŠ¤í…€ Spanìœ¼ë¡œ ì„¸ë°€í•œ ì¶”ì \n",
    "- RAG íŒŒì´í”„ë¼ì¸ì˜ ê° ë‹¨ê³„ë³„ ì„±ëŠ¥ ì¸¡ì •\n",
    "- ë©”íƒ€ë°ì´í„° ë° íƒœê·¸ ì¶”ê°€\n",
    "\n",
    "## í•™ìŠµ í¬ì¸íŠ¸\n",
    "- `mlflow.start_span()` ì‚¬ìš©ë²•\n",
    "- Span Attributes vs Tags\n",
    "- ì¤‘ì²©ëœ Span êµ¬ì¡° ì„¤ê³„\n",
    "- ë””ë²„ê¹… ì›Œí¬í”Œë¡œìš°\n",
    "- ì„±ëŠ¥ ë³‘ëª© ì§€ì  ë¶„ì„"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. ì‚¬ì „ ì¤€ë¹„\n",
    "\n",
    "### âš ï¸ ì£¼ì˜ì‚¬í•­\n",
    "ì´ ë…¸íŠ¸ë¶ì„ ì‹¤í–‰í•˜ê¸° ì „ì—:\n",
    "- **Step 0, 1, 2** ì™„ë£Œ ê¶Œì¥\n",
    "- `.env` íŒŒì¼ ì„¤ì • ì™„ë£Œ\n",
    "- í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ ì„¤ì • ë° íŒ¨í‚¤ì§€ ì„í¬íŠ¸\n",
    "\n",
    "### 1.1 í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ\n",
    "load_dotenv()\n",
    "\n",
    "print(\"âœ… í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ\n",
      "MLflow ë²„ì „: 3.7.0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "from typing import TypedDict, List, Dict, Any\n",
    "from datetime import datetime\n",
    "\n",
    "# MLflow\n",
    "import mlflow\n",
    "from mlflow import MlflowClient\n",
    "from mlflow.entities import SpanType, Document, SpanEvent\n",
    "\n",
    "# LangChain & LangGraph\n",
    "from langchain_aws import ChatBedrock, BedrockEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "print(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ\")\n",
    "print(f\"MLflow ë²„ì „: {mlflow.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MLflow Tracking ì„¤ì •\n",
    "\n",
    "### 2.1 Tracking URI ë° Experiment ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… MLflow ì„¤ì • ì™„ë£Œ\n",
      "ğŸ“Š Experiment Name: rag_agent_custom_spans\n",
      "ğŸ†” Experiment ID: 568044945465180721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hank_kim/Documents/taehan/mlflow-genai-tutorial/.venv/lib/python3.13/site-packages/mlflow/tracking/_tracking_service/utils.py:177: FutureWarning: The filesystem tracking backend (e.g., './mlruns') will be deprecated in February 2026. Consider transitioning to a database backend (e.g., 'sqlite:///mlflow.db') to take advantage of the latest MLflow features. See https://github.com/mlflow/mlflow/issues/18534 for more details and migration guidance.\n",
      "  return FileStore(store_uri, store_uri)\n"
     ]
    }
   ],
   "source": [
    "# Tracking URI ì„¤ì •\n",
    "mlflow.set_tracking_uri(\"./mlruns\")\n",
    "\n",
    "# Experiment ì„¤ì •\n",
    "experiment_name = \"rag_agent_custom_spans\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "\n",
    "print(\"âœ… MLflow ì„¤ì • ì™„ë£Œ\")\n",
    "print(f\"ğŸ“Š Experiment Name: {experiment.name}\")\n",
    "print(f\"ğŸ†” Experiment ID: {experiment.experiment_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ìƒ˜í”Œ ë°ì´í„° ë° Vector Store ì¤€ë¹„\n",
    "\n",
    "### 3.1 ìƒ˜í”Œ ë¬¸ì„œ ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 4ê°œì˜ ìƒ˜í”Œ ë¬¸ì„œ ì¤€ë¹„ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "sample_documents = [\n",
    "    \"\"\"\n",
    "    RAG (Retrieval-Augmented Generation)ëŠ” ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.\n",
    "    RAGëŠ” ì™¸ë¶€ ì§€ì‹ ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ê³ , ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë” ì •í™•í•˜ê³  ì‚¬ì‹¤ì— ê¸°ë°˜í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "    ì´ ë°©ë²•ì€ ëª¨ë¸ì´ í•™ìŠµí•˜ì§€ ì•Šì€ ìµœì‹  ì •ë³´ë‚˜ íŠ¹ì • ë„ë©”ì¸ ì§€ì‹ì„ í™œìš©í•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Vector StoreëŠ” ë¬¸ì„œì˜ ì„ë² ë”© ë²¡í„°ë¥¼ ì €ì¥í•˜ê³  ê²€ìƒ‰í•˜ëŠ” ë°ì´í„°ë² ì´ìŠ¤ì…ë‹ˆë‹¤.\n",
    "    FAISS, Chroma, Pinecone ë“±ì´ ëŒ€í‘œì ì¸ Vector Storeì…ë‹ˆë‹¤.\n",
    "    Vector StoreëŠ” ìœ ì‚¬ë„ ê²€ìƒ‰(Similarity Search)ì„ í†µí•´ ì¿¼ë¦¬ì™€ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œë¥¼ ë¹ ë¥´ê²Œ ì°¾ì•„ëƒ…ë‹ˆë‹¤.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Spanì€ Traceì˜ êµ¬ì„± ìš”ì†Œë¡œ, ë‹¨ì¼ ì‘ì—… ë˜ëŠ” í•¨ìˆ˜ í˜¸ì¶œì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n",
    "    Parent Spanê³¼ Child Spanì˜ ê³„ì¸µ êµ¬ì¡°ë¥¼ í†µí•´ ë³µì¡í•œ ì›Œí¬í”Œë¡œìš°ë¥¼ ì‹œê°í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "    ê° Spanì€ ì‹œì‘ ì‹œê°„, ì¢…ë£Œ ì‹œê°„, ì…ë ¥, ì¶œë ¥, ë©”íƒ€ë°ì´í„°ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    ì»¤ìŠ¤í…€ Spanì„ ì‚¬ìš©í•˜ë©´ ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ íŠ¹ì • ë¶€ë¶„ì„ ì„¸ë°€í•˜ê²Œ ì¶”ì í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "    mlflow.start_span() ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €ë¥¼ ì‚¬ìš©í•˜ì—¬ ìˆ˜ë™ìœ¼ë¡œ Spanì„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "    ì´ë¥¼ í†µí•´ ì„±ëŠ¥ ë³‘ëª© ì§€ì ì„ ì‹ë³„í•˜ê³  ìµœì í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "    \"\"\",\n",
    "]\n",
    "\n",
    "print(f\"âœ… {len(sample_documents)}ê°œì˜ ìƒ˜í”Œ ë¬¸ì„œ ì¤€ë¹„ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Vector Store ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Vector Store ìƒì„± ì™„ë£Œ (4ê°œ chunks)\n",
      "ğŸ“¦ ì„ë² ë”© ëª¨ë¸: amazon.titan-embed-text-v2:0\n"
     ]
    }
   ],
   "source": [
    "# AWS í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ\n",
    "AWS_MODEL_ID = os.getenv(\"AWS_MODEL_ID\")\n",
    "AWS_EMD_MODEL_ID = os.getenv(\"AWS_EMD_MODEL_ID\")\n",
    "AWS_REGION = os.getenv(\"AWS_REGION\", \"us-east-1\")\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ë¶„í• \n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=50,\n",
    ")\n",
    "\n",
    "documents = [Document(page_content=doc.strip()) for doc in sample_documents]\n",
    "split_docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# ì„ë² ë”© ë° Vector Store ìƒì„±\n",
    "embeddings = BedrockEmbeddings(\n",
    "    model_id=AWS_EMD_MODEL_ID,\n",
    "    region_name=AWS_REGION\n",
    ")\n",
    "\n",
    "vector_store = FAISS.from_documents(split_docs, embeddings)\n",
    "\n",
    "print(f\"âœ… Vector Store ìƒì„± ì™„ë£Œ ({len(split_docs)}ê°œ chunks)\")\n",
    "print(f\"ğŸ“¦ ì„ë² ë”© ëª¨ë¸: {AWS_EMD_MODEL_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ì»¤ìŠ¤í…€ Spanì„ ì‚¬ìš©í•œ RAG íŒŒì´í”„ë¼ì¸\n",
    "\n",
    "### 4.1 ê¸°ë³¸ ì»¤ìŠ¤í…€ Span ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… rag_pipeline_with_custom_spans í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "def rag_pipeline_with_custom_spans(query: str, top_k: int = 3) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    ì»¤ìŠ¤í…€ Spanì„ ì‚¬ìš©í•œ RAG íŒŒì´í”„ë¼ì¸\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"ğŸ” Query: {query}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Root Span ì‹œì‘\n",
    "    with mlflow.start_span(name=\"rag_pipeline\", span_type=SpanType.CHAIN) as root_span:\n",
    "        # Root Spanì— ì…ë ¥ ì„¤ì •\n",
    "        root_span.set_inputs({\"query\": query, \"top_k\": top_k})\n",
    "        \n",
    "        # Root Spanì— ë©”íƒ€ë°ì´í„° ì¶”ê°€\n",
    "        root_span.set_attributes({\n",
    "            \"pipeline_version\": \"1.0\",\n",
    "            \"environment\": \"development\",\n",
    "            \"user_id\": \"demo_user\"\n",
    "        })\n",
    "        \n",
    "        print(\"\\nğŸŒ³ Root Span ìƒì„±: rag_pipeline\")\n",
    "        \n",
    "        # Step 1: Document Retrieval\n",
    "        with mlflow.start_span(name=\"document_retrieval\", span_type=SpanType.RETRIEVER) as retrieval_span:\n",
    "            print(\"  â”œâ”€ ğŸ” Retrieval Span ì‹œì‘\")\n",
    "            \n",
    "            retrieval_span.set_inputs({\"query\": query, \"top_k\": top_k})\n",
    "            retrieval_span.set_attributes({\n",
    "                \"retrieval_method\": \"semantic_search\",\n",
    "                \"vector_store_type\": \"FAISS\",\n",
    "                \"embedding_model\": AWS_EMD_MODEL_ID,\n",
    "            })\n",
    "            \n",
    "            # ê²€ìƒ‰ ì‹¤í–‰\n",
    "            start_time = time.time()\n",
    "            retrieved_docs = vector_store.similarity_search(query, k=top_k)\n",
    "            retrieval_time = time.time() - start_time\n",
    "            \n",
    "            # RETRIEVER span ì¶œë ¥ í˜•ì‹ (Document ì—”í‹°í‹° ì‚¬ìš©)\n",
    "            retrieval_outputs = [\n",
    "                Document(\n",
    "                    page_content=doc.page_content,\n",
    "                    metadata={\"doc_uri\": f\"doc_{i}\", \"chunk_id\": f\"chunk_{i}\"}\n",
    "                )\n",
    "                for i, doc in enumerate(retrieved_docs)\n",
    "            ]\n",
    "            \n",
    "            retrieval_span.set_outputs(retrieval_outputs)\n",
    "            retrieval_span.set_attributes({\n",
    "                \"num_docs_retrieved\": len(retrieved_docs),\n",
    "                \"retrieval_time_ms\": retrieval_time * 1000\n",
    "            })\n",
    "            \n",
    "            print(f\"  â”‚  âœ… {len(retrieved_docs)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ ({retrieval_time:.3f}ì´ˆ)\")\n",
    "        \n",
    "        # Step 2: Context Preparation\n",
    "        with mlflow.start_span(name=\"context_preparation\", span_type=\"PREPROCESSING\") as context_span:\n",
    "            print(\"  â”œâ”€ ğŸ“ Context Preparation Span ì‹œì‘\")\n",
    "            \n",
    "            context_span.set_inputs({\"documents\": [doc.page_content for doc in retrieved_docs]})\n",
    "            \n",
    "            # Context ìƒì„±\n",
    "            start_time = time.time()\n",
    "            context = \"\\n\\n\".join([\n",
    "                f\"[ë¬¸ì„œ {i+1}]\\n{doc.page_content}\"\n",
    "                for i, doc in enumerate(retrieved_docs)\n",
    "            ])\n",
    "            context_time = time.time() - start_time\n",
    "            \n",
    "            context_span.set_outputs({\"context\": context})\n",
    "            context_span.set_attributes({\n",
    "                \"context_length\": len(context),\n",
    "                \"num_documents\": len(retrieved_docs),\n",
    "                \"preparation_time_ms\": context_time * 1000\n",
    "            })\n",
    "            \n",
    "            print(f\"  â”‚  âœ… Context ì¤€ë¹„ ì™„ë£Œ ({len(context)} ë¬¸ì)\")\n",
    "        \n",
    "        # Step 3: Prompt Construction\n",
    "        with mlflow.start_span(name=\"prompt_construction\", span_type=\"PREPROCESSING\") as prompt_span:\n",
    "            print(\"  â”œâ”€ ğŸ”¨ Prompt Construction Span ì‹œì‘\")\n",
    "            \n",
    "            prompt_template = ChatPromptTemplate.from_messages([\n",
    "                (\"system\", \"ë‹¹ì‹ ì€ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê³  ê°„ê²°í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.\"),\n",
    "                (\"human\", \"ì»¨í…ìŠ¤íŠ¸:\\n{context}\\n\\nì§ˆë¬¸: {query}\\n\\në‹µë³€:\")\n",
    "            ])\n",
    "            \n",
    "            messages = prompt_template.format_messages(context=context, query=query)\n",
    "            prompt_text = \"\\n\".join([msg.content for msg in messages])\n",
    "            \n",
    "            prompt_span.set_inputs({\"context\": context, \"query\": query})\n",
    "            prompt_span.set_outputs({\"prompt\": prompt_text})\n",
    "            prompt_span.set_attributes({\n",
    "                \"prompt_length\": len(prompt_text),\n",
    "                \"template_type\": \"chat\"\n",
    "            })\n",
    "            \n",
    "            print(f\"  â”‚  âœ… Prompt ìƒì„± ì™„ë£Œ ({len(prompt_text)} ë¬¸ì)\")\n",
    "        \n",
    "        # Step 4: Answer Generation\n",
    "        with mlflow.start_span(name=\"answer_generation\", span_type=SpanType.LLM) as generation_span:\n",
    "            print(\"  â””â”€ âœï¸  Generation Span ì‹œì‘\")\n",
    "            \n",
    "            generation_span.set_inputs({\"prompt\": prompt_text})\n",
    "            generation_span.set_attributes({\n",
    "                \"llm_model\": AWS_MODEL_ID,\n",
    "                \"temperature\": 0.7,\n",
    "                \"max_tokens\": 500\n",
    "            })\n",
    "            \n",
    "            # LLM í˜¸ì¶œ (AWS Bedrock)\n",
    "            llm = ChatBedrock(\n",
    "                model_id=AWS_MODEL_ID,\n",
    "                region_name=AWS_REGION,\n",
    "                model_kwargs={\"temperature\": 0.7}\n",
    "            )\n",
    "            \n",
    "            start_time = time.time()\n",
    "            response = llm.invoke(messages)\n",
    "            generation_time = time.time() - start_time\n",
    "            \n",
    "            answer = response.content\n",
    "            \n",
    "            generation_span.set_outputs({\"answer\": answer})\n",
    "            generation_span.set_attributes({\n",
    "                \"answer_length\": len(answer),\n",
    "                \"generation_time_ms\": generation_time * 1000\n",
    "            })\n",
    "            \n",
    "            print(f\"     âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ ({generation_time:.3f}ì´ˆ)\")\n",
    "        \n",
    "        # Root Span ì¶œë ¥ ì„¤ì •\n",
    "        result = {\n",
    "            \"query\": query,\n",
    "            \"answer\": answer,\n",
    "            \"num_docs_retrieved\": len(retrieved_docs),\n",
    "            \"retrieval_time\": retrieval_time,\n",
    "            \"generation_time\": generation_time\n",
    "        }\n",
    "        \n",
    "        root_span.set_outputs(result)\n",
    "        \n",
    "        print(\"\\nâœ… RAG íŒŒì´í”„ë¼ì¸ ì™„ë£Œ\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        return result\n",
    "\n",
    "print(\"âœ… rag_pipeline_with_custom_spans í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 ì»¤ìŠ¤í…€ Span ì‹¤í–‰ í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ” Query: ì»¤ìŠ¤í…€ Spanì´ ë¬´ì—‡ì´ê³  ì™œ ìœ ìš©í•œê°€ìš”?\n",
      "================================================================================\n",
      "\n",
      "ğŸŒ³ Root Span ìƒì„±: rag_pipeline\n",
      "  â”œâ”€ ğŸ” Retrieval Span ì‹œì‘\n",
      "  â”‚  âœ… 3ê°œ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ (0.084ì´ˆ)\n",
      "  â”œâ”€ ğŸ“ Context Preparation Span ì‹œì‘\n",
      "  â”‚  âœ… Context ì¤€ë¹„ ì™„ë£Œ (501 ë¬¸ì)\n",
      "  â”œâ”€ ğŸ”¨ Prompt Construction Span ì‹œì‘\n",
      "  â”‚  âœ… Prompt ìƒì„± ì™„ë£Œ (606 ë¬¸ì)\n",
      "  â””â”€ âœï¸  Generation Span ì‹œì‘\n",
      "     âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ (5.542ì´ˆ)\n",
      "\n",
      "âœ… RAG íŒŒì´í”„ë¼ì¸ ì™„ë£Œ\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "ğŸ“‹ ì‹¤í–‰ ê²°ê³¼\n",
      "================================================================================\n",
      "\n",
      "â“ ì§ˆë¬¸: ì»¤ìŠ¤í…€ Spanì´ ë¬´ì—‡ì´ê³  ì™œ ìœ ìš©í•œê°€ìš”?\n",
      "\n",
      "ğŸ’¬ ë‹µë³€:\n",
      "ì»¤ìŠ¤í…€ Spanì€ ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ íŠ¹ì • ë¶€ë¶„ì„ ì„¸ë°€í•˜ê²Œ ì¶”ì í•˜ê¸° ìœ„í•´ ìˆ˜ë™ìœ¼ë¡œ ìƒì„±í•˜ëŠ” Spanì…ë‹ˆë‹¤.\n",
      "\n",
      "**ì»¤ìŠ¤í…€ Spanì˜ íŠ¹ì§•:**\n",
      "- `mlflow.start_span()` ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €ë¥¼ ì‚¬ìš©í•˜ì—¬ ìˆ˜ë™ìœ¼ë¡œ ìƒì„±\n",
      "- Traceì˜ êµ¬ì„± ìš”ì†Œë¡œì„œ ë‹¨ì¼ ì‘ì—… ë˜ëŠ” í•¨ìˆ˜ í˜¸ì¶œì„ ë‚˜íƒ€ëƒ„\n",
      "- ì‹œì‘ ì‹œê°„, ì¢…ë£Œ ì‹œê°„, ì…ë ¥, ì¶œë ¥, ë©”íƒ€ë°ì´í„°ë¥¼ í¬í•¨\n",
      "\n",
      "**ìœ ìš©í•œ ì´ìœ :**\n",
      "1. **ì„¸ë°€í•œ ì¶”ì **: ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ íŠ¹ì • ë¶€ë¶„ì„ ìƒì„¸í•˜ê²Œ ëª¨ë‹ˆí„°ë§\n",
      "2. **ì„±ëŠ¥ ìµœì í™”**: ë³‘ëª© ì§€ì ì„ ì‹ë³„í•˜ê³  ìµœì í™” ê°€ëŠ¥\n",
      "3. **ì›Œí¬í”Œë¡œìš° ì‹œê°í™”**: Parent-Child ê³„ì¸µ êµ¬ì¡°ë¥¼ í†µí•´ ë³µì¡í•œ ì›Œí¬í”Œë¡œìš°ë¥¼ ì‹œê°ì ìœ¼ë¡œ ì´í•´\n",
      "\n",
      "ì»¤ìŠ¤í…€ Spanì„ í™œìš©í•˜ë©´ ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ì‹¤í–‰ íë¦„ê³¼ ì„±ëŠ¥ì„ ë” ê¹Šì´ ìˆê²Œ ë¶„ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ğŸ“Š ë©”íŠ¸ë¦­:\n",
      "  â€¢ ê²€ìƒ‰ ì‹œê°„: 0.084ì´ˆ\n",
      "  â€¢ ìƒì„± ì‹œê°„: 5.542ì´ˆ\n",
      "  â€¢ ê²€ìƒ‰ ë¬¸ì„œ: 3ê°œ\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬\n",
    "test_query = \"ì»¤ìŠ¤í…€ Spanì´ ë¬´ì—‡ì´ê³  ì™œ ìœ ìš©í•œê°€ìš”?\"\n",
    "\n",
    "# íŒŒì´í”„ë¼ì¸ ì‹¤í–‰\n",
    "result = rag_pipeline_with_custom_spans(test_query, top_k=3)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“‹ ì‹¤í–‰ ê²°ê³¼\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nâ“ ì§ˆë¬¸: {result['query']}\")\n",
    "print(f\"\\nğŸ’¬ ë‹µë³€:\\n{result['answer']}\")\n",
    "print(f\"\\nğŸ“Š ë©”íŠ¸ë¦­:\")\n",
    "print(f\"  â€¢ ê²€ìƒ‰ ì‹œê°„: {result['retrieval_time']:.3f}ì´ˆ\")\n",
    "print(f\"  â€¢ ìƒì„± ì‹œê°„: {result['generation_time']:.3f}ì´ˆ\")\n",
    "print(f\"  â€¢ ê²€ìƒ‰ ë¬¸ì„œ: {result['num_docs_retrieved']}ê°œ\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Trace Tags ë° Attributes í™œìš©\n",
    "\n",
    "### 5.1 Trace ë ˆë²¨ Tags ì¶”ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Tagsê°€ í¬í•¨ëœ Trace ìƒì„± ì™„ë£Œ\n",
      "ğŸ’¬ ë‹µë³€: RAG(Retrieval-Augmented Generation)ëŠ” **ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ê¸°ìˆ **ì…ë‹ˆë‹¤.\n",
      "\n",
      "ì£¼ìš” íŠ¹ì§•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
      "\n",
      "1. **ì™¸ë¶€...\n"
     ]
    }
   ],
   "source": [
    "def rag_pipeline_with_tags(query: str, user_id: str, session_id: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Trace ë ˆë²¨ íƒœê·¸ë¥¼ í¬í•¨í•œ RAG íŒŒì´í”„ë¼ì¸\n",
    "    \"\"\"\n",
    "    with mlflow.start_span(name=\"rag_pipeline\", span_type=SpanType.CHAIN) as root_span:\n",
    "        # Trace ID ê°€ì ¸ì˜¤ê¸°\n",
    "        trace_id = root_span.request_id\n",
    "        \n",
    "        # Trace ë ˆë²¨ íƒœê·¸ ì„¤ì •\n",
    "        mlflow.set_trace_tag(trace_id, \"user_id\", user_id)\n",
    "        mlflow.set_trace_tag(trace_id, \"session_id\", session_id)\n",
    "        mlflow.set_trace_tag(trace_id, \"environment\", \"production\")\n",
    "        mlflow.set_trace_tag(trace_id, \"user_segment\", \"premium\")\n",
    "        \n",
    "        root_span.set_inputs({\"query\": query})\n",
    "        \n",
    "        # ê²€ìƒ‰\n",
    "        with mlflow.start_span(name=\"retrieval\", span_type=SpanType.RETRIEVER) as retrieval_span:\n",
    "            docs = vector_store.similarity_search(query, k=3)\n",
    "            \n",
    "            retrieval_outputs = [\n",
    "                Document(page_content=doc.page_content, metadata={\"doc_uri\": f\"doc_{i}\"})\n",
    "                for i, doc in enumerate(docs)\n",
    "            ]\n",
    "            retrieval_span.set_outputs(retrieval_outputs)\n",
    "        \n",
    "        # ìƒì„±\n",
    "        with mlflow.start_span(name=\"generation\", span_type=SpanType.LLM) as gen_span:\n",
    "            context = \"\\n\".join([doc.page_content for doc in docs])\n",
    "            \n",
    "            # LLM í˜¸ì¶œ (AWS Bedrock)\n",
    "            llm = ChatBedrock(\n",
    "                model_id=AWS_MODEL_ID,\n",
    "                region_name=AWS_REGION,\n",
    "                model_kwargs={\"temperature\": 0.7}\n",
    "            )\n",
    "            prompt = f\"ì»¨í…ìŠ¤íŠ¸: {context}\\n\\nì§ˆë¬¸: {query}\\n\\në‹µë³€:\"\n",
    "            \n",
    "            response = llm.invoke(prompt)\n",
    "            answer = response.content\n",
    "            \n",
    "            gen_span.set_outputs({\"answer\": answer})\n",
    "        \n",
    "        result = {\"query\": query, \"answer\": answer}\n",
    "        root_span.set_outputs(result)\n",
    "        \n",
    "        return result\n",
    "\n",
    "# íƒœê·¸ì™€ í•¨ê»˜ ì‹¤í–‰\n",
    "result_with_tags = rag_pipeline_with_tags(\n",
    "    query=\"RAGëŠ” ë¬´ì—‡ì¸ê°€ìš”?\",\n",
    "    user_id=\"user_12345\",\n",
    "    session_id=\"session_abc\"\n",
    ")\n",
    "\n",
    "print(\"âœ… Tagsê°€ í¬í•¨ëœ Trace ìƒì„± ì™„ë£Œ\")\n",
    "print(f\"ğŸ’¬ ë‹µë³€: {result_with_tags['answer'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Attributes vs Tags ë¹„êµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ“š Attributes vs Tags ë¹„êµ\n",
      "================================================================================\n",
      "íŠ¹ì§•                   | Attributes                          | Tags                               \n",
      "-------------------- | ----------------------------------- | -----------------------------------\n",
      "ë²”ìœ„                   | Span ë ˆë²¨                             | Trace ë ˆë²¨                           \n",
      "ì„¤ì • ë°©ë²•                | span.set_attributes()               | mlflow.set_trace_tag()             \n",
      "ìš©ë„                   | ì‹¤í–‰ ë©”íƒ€ë°ì´í„°, ì„¤ì •ê°’                       | ì‚¬ìš©ì/ì„¸ì…˜ ì •ë³´, ì¹´í…Œê³ ë¦¬                    \n",
      "ë³€ê²½ ê°€ëŠ¥                | No (ë¶ˆë³€)                             | Yes (UIì—ì„œ ìˆ˜ì • ê°€ëŠ¥)                   \n",
      "ê²€ìƒ‰                   | Span í•„í„°ë§                            | Trace í•„í„°ë§                          \n",
      "ì˜ˆì‹œ                   | model_name, temperature             | user_id, environment               \n",
      "\n",
      "================================================================================\n",
      "ğŸ’¡ ê¶Œì¥ ì‚¬ìš©ë²•:\n",
      "  â€¢ Attributes: í•¨ìˆ˜ ì‹¤í–‰ê³¼ ì§ì ‘ ê´€ë ¨ëœ ë©”íƒ€ë°ì´í„°\n",
      "  â€¢ Tags: ë¹„ì¦ˆë‹ˆìŠ¤ ì»¨í…ìŠ¤íŠ¸, ì‚¬ìš©ì ì •ë³´, í™˜ê²½ ì •ë³´\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“š Attributes vs Tags ë¹„êµ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "comparison = [\n",
    "    [\"íŠ¹ì§•\", \"Attributes\", \"Tags\"],\n",
    "    [\"-\" * 20, \"-\" * 35, \"-\" * 35],\n",
    "    [\"ë²”ìœ„\", \"Span ë ˆë²¨\", \"Trace ë ˆë²¨\"],\n",
    "    [\"ì„¤ì • ë°©ë²•\", \"span.set_attributes()\", \"mlflow.set_trace_tag()\"],\n",
    "    [\"ìš©ë„\", \"ì‹¤í–‰ ë©”íƒ€ë°ì´í„°, ì„¤ì •ê°’\", \"ì‚¬ìš©ì/ì„¸ì…˜ ì •ë³´, ì¹´í…Œê³ ë¦¬\"],\n",
    "    [\"ë³€ê²½ ê°€ëŠ¥\", \"No (ë¶ˆë³€)\", \"Yes (UIì—ì„œ ìˆ˜ì • ê°€ëŠ¥)\"],\n",
    "    [\"ê²€ìƒ‰\", \"Span í•„í„°ë§\", \"Trace í•„í„°ë§\"],\n",
    "    [\"ì˜ˆì‹œ\", \"model_name, temperature\", \"user_id, environment\"],\n",
    "]\n",
    "\n",
    "for row in comparison:\n",
    "    print(f\"{row[0]:<20} | {row[1]:<35} | {row[2]:<35}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ’¡ ê¶Œì¥ ì‚¬ìš©ë²•:\")\n",
    "print(\"  â€¢ Attributes: í•¨ìˆ˜ ì‹¤í–‰ê³¼ ì§ì ‘ ê´€ë ¨ëœ ë©”íƒ€ë°ì´í„°\")\n",
    "print(\"  â€¢ Tags: ë¹„ì¦ˆë‹ˆìŠ¤ ì»¨í…ìŠ¤íŠ¸, ì‚¬ìš©ì ì •ë³´, í™˜ê²½ ì •ë³´\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ì—¬ëŸ¬ ì¿¼ë¦¬ë¡œ ì„±ëŠ¥ ë¹„êµ\n",
    "\n",
    "### 6.1 ë°°ì¹˜ ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ”„ ë°°ì¹˜ ì‹¤í–‰ ì‹œì‘\n",
      "================================================================================\n",
      "\n",
      "[1/4] ì²˜ë¦¬ ì¤‘...\n",
      "\n",
      "================================================================================\n",
      "ğŸ” Query: RAGê°€ ë¬´ì—‡ì¸ê°€ìš”?\n",
      "================================================================================\n",
      "\n",
      "ğŸŒ³ Root Span ìƒì„±: rag_pipeline\n",
      "  â”œâ”€ ğŸ” Retrieval Span ì‹œì‘\n",
      "  â”‚  âœ… 3ê°œ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ (0.120ì´ˆ)\n",
      "  â”œâ”€ ğŸ“ Context Preparation Span ì‹œì‘\n",
      "  â”‚  âœ… Context ì¤€ë¹„ ì™„ë£Œ (518 ë¬¸ì)\n",
      "  â”œâ”€ ğŸ”¨ Prompt Construction Span ì‹œì‘\n",
      "  â”‚  âœ… Prompt ìƒì„± ì™„ë£Œ (611 ë¬¸ì)\n",
      "  â””â”€ âœï¸  Generation Span ì‹œì‘\n",
      "     âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ (4.534ì´ˆ)\n",
      "\n",
      "âœ… RAG íŒŒì´í”„ë¼ì¸ ì™„ë£Œ\n",
      "================================================================================\n",
      "\n",
      "[2/4] ì²˜ë¦¬ ì¤‘...\n",
      "\n",
      "================================================================================\n",
      "ğŸ” Query: Vector StoreëŠ” ì–´ë–»ê²Œ ì‘ë™í•˜ë‚˜ìš”?\n",
      "================================================================================\n",
      "\n",
      "ğŸŒ³ Root Span ìƒì„±: rag_pipeline\n",
      "  â”œâ”€ ğŸ” Retrieval Span ì‹œì‘\n",
      "  â”‚  âœ… 3ê°œ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ (0.099ì´ˆ)\n",
      "  â”œâ”€ ğŸ“ Context Preparation Span ì‹œì‘\n",
      "  â”‚  âœ… Context ì¤€ë¹„ ì™„ë£Œ (501 ë¬¸ì)\n",
      "  â”œâ”€ ğŸ”¨ Prompt Construction Span ì‹œì‘\n",
      "  â”‚  âœ… Prompt ìƒì„± ì™„ë£Œ (607 ë¬¸ì)\n",
      "  â””â”€ âœï¸  Generation Span ì‹œì‘\n",
      "     âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ (5.182ì´ˆ)\n",
      "\n",
      "âœ… RAG íŒŒì´í”„ë¼ì¸ ì™„ë£Œ\n",
      "================================================================================\n",
      "\n",
      "[3/4] ì²˜ë¦¬ ì¤‘...\n",
      "\n",
      "================================================================================\n",
      "ğŸ” Query: Spanì˜ ê³„ì¸µ êµ¬ì¡°ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.\n",
      "================================================================================\n",
      "\n",
      "ğŸŒ³ Root Span ìƒì„±: rag_pipeline\n",
      "  â”œâ”€ ğŸ” Retrieval Span ì‹œì‘\n",
      "  â”‚  âœ… 3ê°œ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ (0.131ì´ˆ)\n",
      "  â”œâ”€ ğŸ“ Context Preparation Span ì‹œì‘\n",
      "  â”‚  âœ… Context ì¤€ë¹„ ì™„ë£Œ (518 ë¬¸ì)\n",
      "  â”œâ”€ ğŸ”¨ Prompt Construction Span ì‹œì‘\n",
      "  â”‚  âœ… Prompt ìƒì„± ì™„ë£Œ (623 ë¬¸ì)\n",
      "  â””â”€ âœï¸  Generation Span ì‹œì‘\n",
      "     âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ (5.203ì´ˆ)\n",
      "\n",
      "âœ… RAG íŒŒì´í”„ë¼ì¸ ì™„ë£Œ\n",
      "================================================================================\n",
      "\n",
      "[4/4] ì²˜ë¦¬ ì¤‘...\n",
      "\n",
      "================================================================================\n",
      "ğŸ” Query: ì»¤ìŠ¤í…€ Spanì˜ ì¥ì ì€ ë¬´ì—‡ì¸ê°€ìš”?\n",
      "================================================================================\n",
      "\n",
      "ğŸŒ³ Root Span ìƒì„±: rag_pipeline\n",
      "  â”œâ”€ ğŸ” Retrieval Span ì‹œì‘\n",
      "  â”‚  âœ… 3ê°œ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ (0.137ì´ˆ)\n",
      "  â”œâ”€ ğŸ“ Context Preparation Span ì‹œì‘\n",
      "  â”‚  âœ… Context ì¤€ë¹„ ì™„ë£Œ (501 ë¬¸ì)\n",
      "  â”œâ”€ ğŸ”¨ Prompt Construction Span ì‹œì‘\n",
      "  â”‚  âœ… Prompt ìƒì„± ì™„ë£Œ (603 ë¬¸ì)\n",
      "  â””â”€ âœï¸  Generation Span ì‹œì‘\n",
      "     âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ (4.967ì´ˆ)\n",
      "\n",
      "âœ… RAG íŒŒì´í”„ë¼ì¸ ì™„ë£Œ\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "âœ… ì´ 4ê°œ ì¿¼ë¦¬ ì²˜ë¦¬ ì™„ë£Œ\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ì—¬ëŸ¬ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬\n",
    "test_queries = [\n",
    "    \"RAGê°€ ë¬´ì—‡ì¸ê°€ìš”?\",\n",
    "    \"Vector StoreëŠ” ì–´ë–»ê²Œ ì‘ë™í•˜ë‚˜ìš”?\",\n",
    "    \"Spanì˜ ê³„ì¸µ êµ¬ì¡°ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.\",\n",
    "    \"ì»¤ìŠ¤í…€ Spanì˜ ì¥ì ì€ ë¬´ì—‡ì¸ê°€ìš”?\",\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ”„ ë°°ì¹˜ ì‹¤í–‰ ì‹œì‘\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = []\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\n[{i}/{len(test_queries)}] ì²˜ë¦¬ ì¤‘...\")\n",
    "    result = rag_pipeline_with_custom_spans(query, top_k=3)\n",
    "    results.append(result)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"âœ… ì´ {len(results)}ê°œ ì¿¼ë¦¬ ì²˜ë¦¬ ì™„ë£Œ\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Trace ê²€ìƒ‰ ë° ë¶„ì„\n",
    "\n",
    "### 7.1 ìƒì„±ëœ Trace ê²€ìƒ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š ê²€ìƒ‰ëœ Trace: 7ê°œ\n",
      "\n",
      "================================================================================\n",
      "Request ID                               ì‹¤í–‰ ì‹œê°„           Spans     \n",
      "================================================================================\n",
      "tr-49b7551534d275297ac8a68f4ba36f50      5209ms          5         \n",
      "tr-21ee7b3157088706bca7d1ffdd79aa5a      5415ms          5         \n",
      "tr-6fdb25a9610693b99835881f9b962696      5380ms          5         \n",
      "tr-4df891d983d3a03a50721744da243ebe      4724ms          5         \n",
      "tr-818d768c4e5fb30fdb7c3a21158cf117      5574ms          3         \n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v7/jvdq74bd213fzs8k9cmpl_7w0000gn/T/ipykernel_7078/3748418091.py:2: FutureWarning: Parameter 'experiment_ids' is deprecated. Please use 'locations' instead.\n",
      "  traces = mlflow.search_traces(\n"
     ]
    }
   ],
   "source": [
    "# Trace ê²€ìƒ‰\n",
    "traces = mlflow.search_traces(\n",
    "    experiment_ids=[experiment.experiment_id],\n",
    "    max_results=10,\n",
    "    order_by=[\"timestamp_ms DESC\"],\n",
    "    return_type=\"list\",\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ“Š ê²€ìƒ‰ëœ Trace: {len(traces)}ê°œ\\n\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Request ID':<40} {'ì‹¤í–‰ ì‹œê°„':<15} {'Spans':<10}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for trace in traces[:5]:  # ìµœê·¼ 5ê°œë§Œ ì¶œë ¥\n",
    "    request_id = trace.info.request_id[:36]\n",
    "    exec_time = f\"{trace.info.execution_time_ms:.0f}ms\" if trace.info.execution_time_ms else \"N/A\"\n",
    "    num_spans = len(trace.data.spans) if trace.data.spans else 0\n",
    "    \n",
    "    print(f\"{request_id:<40} {exec_time:<15} {num_spans:<10}\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Spanë³„ ì„±ëŠ¥ ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "â±ï¸  Spanë³„ ì‹¤í–‰ ì‹œê°„ ë¶„ì„\n",
      "================================================================================\n",
      "\n",
      "Span Name                      Type            Duration (ms)  \n",
      "--------------------------------------------------------------------------------\n",
      "rag_pipeline                   CHAIN                5209.13\n",
      "answer_generation              LLM                  5069.03\n",
      "document_retrieval             RETRIEVER             138.13\n",
      "prompt_construction            PREPROCESSING           0.68\n",
      "context_preparation            PREPROCESSING           0.19\n",
      "\n",
      "================================================================================\n",
      "ğŸŒ ê°€ì¥ ëŠë¦° Span: rag_pipeline (5209.13ms)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "def analyze_span_performance(trace):\n",
    "    \"\"\"\n",
    "    Traceì˜ ê° Span ì„±ëŠ¥ ë¶„ì„\n",
    "    \"\"\"\n",
    "    if not trace.data.spans:\n",
    "        return\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"â±ï¸  Spanë³„ ì‹¤í–‰ ì‹œê°„ ë¶„ì„\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    span_times = []\n",
    "    \n",
    "    for span in trace.data.spans:\n",
    "        duration_ms = (span.end_time_ns - span.start_time_ns) / 1_000_000\n",
    "        span_times.append({\n",
    "            \"name\": span.name,\n",
    "            \"type\": span.span_type,\n",
    "            \"duration_ms\": duration_ms\n",
    "        })\n",
    "    \n",
    "    # ì‹¤í–‰ ì‹œê°„ ìˆœ ì •ë ¬\n",
    "    span_times.sort(key=lambda x: x[\"duration_ms\"], reverse=True)\n",
    "    \n",
    "    print(f\"\\n{'Span Name':<30} {'Type':<15} {'Duration (ms)':<15}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for span_info in span_times:\n",
    "        print(f\"{span_info['name']:<30} {span_info['type']:<15} {span_info['duration_ms']:>12.2f}\")\n",
    "    \n",
    "    # ê°€ì¥ ëŠë¦° Span\n",
    "    slowest = span_times[0]\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"ğŸŒ ê°€ì¥ ëŠë¦° Span: {slowest['name']} ({slowest['duration_ms']:.2f}ms)\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "\n",
    "if traces:\n",
    "    analyze_span_performance(traces[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 ì„±ëŠ¥ ë³‘ëª© ì§€ì  ì‹ë³„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ” ì„±ëŠ¥ ë³‘ëª© ì§€ì  ë¶„ì„\n",
      "================================================================================\n",
      "\n",
      "Span Name                      Avg (ms)     Max (ms)     Count   \n",
      "--------------------------------------------------------------------------------\n",
      "rag_pipeline                      5260.73    5574.51      5\n",
      "document_retrieval                 122.29     138.13      4\n",
      "context_preparation                  0.19       0.33      4\n",
      "prompt_construction                  0.44       0.68      4\n",
      "answer_generation                 5058.41    5281.18      4\n",
      "retrieval                          103.86     103.86      1\n",
      "generation                        5470.25    5470.25      1\n",
      "\n",
      "================================================================================\n",
      "âš ï¸  ë³‘ëª© ì§€ì  ë°œê²¬ (ì„ê³„ê°’: 500ms):\n",
      "  â€¢ generation: 5470.25ms (í‰ê· )\n",
      "  â€¢ rag_pipeline: 5260.73ms (í‰ê· )\n",
      "  â€¢ answer_generation: 5058.41ms (í‰ê· )\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "def identify_bottlenecks(traces_list, threshold_ms=1000):\n",
    "    \"\"\"\n",
    "    ì—¬ëŸ¬ Traceì—ì„œ ì„±ëŠ¥ ë³‘ëª© ì§€ì  ì‹ë³„\n",
    "    \"\"\"\n",
    "    span_stats = {}\n",
    "    \n",
    "    for trace in traces_list:\n",
    "        if not trace.data.spans:\n",
    "            continue\n",
    "        \n",
    "        for span in trace.data.spans:\n",
    "            duration_ms = (span.end_time_ns - span.start_time_ns) / 1_000_000\n",
    "            \n",
    "            if span.name not in span_stats:\n",
    "                span_stats[span.name] = []\n",
    "            \n",
    "            span_stats[span.name].append(duration_ms)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ğŸ” ì„±ëŠ¥ ë³‘ëª© ì§€ì  ë¶„ì„\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\n{'Span Name':<30} {'Avg (ms)':<12} {'Max (ms)':<12} {'Count':<8}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    bottlenecks = []\n",
    "    \n",
    "    for span_name, durations in span_stats.items():\n",
    "        avg_time = sum(durations) / len(durations)\n",
    "        max_time = max(durations)\n",
    "        count = len(durations)\n",
    "        \n",
    "        print(f\"{span_name:<30} {avg_time:>10.2f} {max_time:>10.2f} {count:>6}\")\n",
    "        \n",
    "        if avg_time > threshold_ms:\n",
    "            bottlenecks.append((span_name, avg_time))\n",
    "    \n",
    "    if bottlenecks:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"âš ï¸  ë³‘ëª© ì§€ì  ë°œê²¬ (ì„ê³„ê°’: {threshold_ms}ms):\")\n",
    "        for span_name, avg_time in sorted(bottlenecks, key=lambda x: x[1], reverse=True):\n",
    "            print(f\"  â€¢ {span_name}: {avg_time:.2f}ms (í‰ê· )\")\n",
    "    else:\n",
    "        print(f\"\\nâœ… ëª¨ë“  Spanì´ ì„ê³„ê°’ {threshold_ms}ms ì´ë‚´ì…ë‹ˆë‹¤.\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "\n",
    "\n",
    "if traces:\n",
    "    identify_bottlenecks(traces[:5], threshold_ms=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Span ê³„ì¸µ êµ¬ì¡° ì‹œê°í™”\n",
    "\n",
    "### 8.1 íŠ¸ë¦¬ í˜•íƒœ ì¶œë ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸŒ³ Span ê³„ì¸µ êµ¬ì¡°\n",
      "================================================================================\n",
      "\n",
      "ğŸŒ³ rag_pipeline [CHAIN] (5209.13ms)\n",
      "â”œâ”€ document_retrieval [RETRIEVER] (138.13ms)\n",
      "      ğŸ’¡ num_docs_retrieved: 3\n",
      "      ğŸ’¡ retrieval_method: semantic_search\n",
      "â”œâ”€ context_preparation [PREPROCESSING] (0.19ms)\n",
      "â”œâ”€ prompt_construction [PREPROCESSING] (0.68ms)\n",
      "â””â”€ answer_generation [LLM] (5069.03ms)\n",
      "      ğŸ’¡ llm_model: global.anthropic.claude-sonnet-4-5-20250929-v1:0\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "def visualize_span_tree(trace):\n",
    "    \"\"\"\n",
    "    Span ê³„ì¸µ êµ¬ì¡°ë¥¼ íŠ¸ë¦¬ í˜•íƒœë¡œ ì‹œê°í™”\n",
    "    \"\"\"\n",
    "    if not trace.data.spans:\n",
    "        print(\"Spanì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "    \n",
    "    # Parent-Child ê´€ê³„ êµ¬ì¶•\n",
    "    span_map = {span.span_id: span for span in trace.data.spans}\n",
    "    root_spans = [span for span in trace.data.spans if span.parent_id is None]\n",
    "    \n",
    "    def print_span_recursive(span, level=0, is_last=False):\n",
    "        # ì¸ë´íŠ¸ ë° ì—°ê²°ì„ \n",
    "        if level == 0:\n",
    "            prefix = \"ğŸŒ³ \"\n",
    "        else:\n",
    "            prefix = \"   \" * (level - 1)\n",
    "            prefix += \"â””â”€ \" if is_last else \"â”œâ”€ \"\n",
    "        \n",
    "        # Span ì •ë³´\n",
    "        duration_ms = (span.end_time_ns - span.start_time_ns) / 1_000_000\n",
    "        span_type = span.span_type if span.span_type else \"UNKNOWN\"\n",
    "        \n",
    "        print(f\"{prefix}{span.name} [{span_type}] ({duration_ms:.2f}ms)\")\n",
    "        \n",
    "        # Attributes ì¶œë ¥ (ì£¼ìš” í•­ëª©ë§Œ)\n",
    "        if span.attributes:\n",
    "            indent = \"   \" * level\n",
    "            for key in [\"llm_model\", \"num_docs_retrieved\", \"retrieval_method\"]:\n",
    "                if key in span.attributes:\n",
    "                    print(f\"{indent}   ğŸ’¡ {key}: {span.attributes[key]}\")\n",
    "        \n",
    "        # Child Spans ì¬ê·€ ì¶œë ¥\n",
    "        children = [s for s in trace.data.spans if s.parent_id == span.span_id]\n",
    "        for i, child in enumerate(children):\n",
    "            is_last_child = (i == len(children) - 1)\n",
    "            print_span_recursive(child, level + 1, is_last_child)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ğŸŒ³ Span ê³„ì¸µ êµ¬ì¡°\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    for root_span in root_spans:\n",
    "        print_span_recursive(root_span)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "\n",
    "if traces:\n",
    "    visualize_span_tree(traces[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ê³ ê¸‰: Span Events ì‚¬ìš©\n",
    "\n",
    "### 9.1 ì¤‘ìš” ì´ë²¤íŠ¸ ë¡œê¹…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Eventsê°€ í¬í•¨ëœ Trace ìƒì„± ì™„ë£Œ\n",
      "ğŸ’¬ ë‹µë³€: Vector StoreëŠ” **ë¬¸ì„œì˜ ì„ë² ë”© ë²¡í„°ë¥¼ ì €ì¥í•˜ê³  ê²€ìƒ‰í•˜ëŠ” ë°ì´í„°ë² ì´ìŠ¤**ì…ë‹ˆë‹¤.\n",
      "\n",
      "ì£¼ìš” íŠ¹ì§•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
      "\n",
      "## ëŒ€í‘œì ì¸ Vector Store\n",
      "- **FAISS...\n"
     ]
    }
   ],
   "source": [
    "def rag_pipeline_with_events(query: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Span Eventsë¥¼ ì‚¬ìš©í•œ RAG íŒŒì´í”„ë¼ì¸\n",
    "    \"\"\"\n",
    "    with mlflow.start_span(name=\"rag_pipeline\", span_type=SpanType.CHAIN) as root_span:\n",
    "        root_span.set_inputs({\"query\": query})\n",
    "\n",
    "        # Event: íŒŒì´í”„ë¼ì¸ ì‹œì‘\n",
    "        # ì´ë²¤íŠ¸ ìƒì„±\n",
    "        pipeline_started_event = SpanEvent(\n",
    "            name=\"pipeline_started\",\n",
    "            timestamp=datetime.now().isoformat(), \n",
    "            attributes={\"info\": \"pipeline ì‹œì‘\"}\n",
    "        )\n",
    "        root_span.add_event(pipeline_started_event)\n",
    "        \n",
    "        with mlflow.start_span(name=\"retrieval\", span_type=SpanType.RETRIEVER) as retrieval_span:\n",
    "            # Event: ê²€ìƒ‰ ì‹œì‘\n",
    "            # ì´ë²¤íŠ¸ ìƒì„±\n",
    "            search_initiated_event = SpanEvent(\n",
    "                name=\"search_initiated\",\n",
    "                attributes={\"query\": query}\n",
    "            )\n",
    "            retrieval_span.add_event(search_initiated_event)\n",
    "            \n",
    "            docs = vector_store.similarity_search(query, k=3)\n",
    "            \n",
    "            # Event: ê²€ìƒ‰ ì™„ë£Œ\n",
    "            search_completed_event = SpanEvent(\n",
    "                name=\"search_completed\",\n",
    "                attributes={\n",
    "                \"num_docs\": len(docs),\n",
    "                \"doc_ids\": [f\"doc_{i}\" for i in range(len(docs))]\n",
    "                }\n",
    "            )\n",
    "            retrieval_span.add_event(search_completed_event)\n",
    "            \n",
    "            retrieval_outputs = [\n",
    "                Document(page_content=doc.page_content, metadata={\"doc_uri\": f\"doc_{i}\"})\n",
    "                for i, doc in enumerate(docs)\n",
    "            ]\n",
    "            retrieval_span.set_outputs(retrieval_outputs)\n",
    "        \n",
    "        with mlflow.start_span(name=\"generation\", span_type=SpanType.LLM) as gen_span:\n",
    "            # Event: LLM í˜¸ì¶œ ì‹œì‘\n",
    "            gen_span.add_event(SpanEvent(\"llm_call_started\", attributes={\"model\": AWS_MODEL_ID}))\n",
    "            \n",
    "            context = \"\\n\".join([doc.page_content for doc in docs])\n",
    "            \n",
    "            # LLM í˜¸ì¶œ (AWS Bedrock)\n",
    "            llm = ChatBedrock(\n",
    "                model_id=AWS_MODEL_ID,\n",
    "                region_name=AWS_REGION,\n",
    "                model_kwargs={\"temperature\": 0.7}\n",
    "            )\n",
    "            \n",
    "            response = llm.invoke(f\"Context: {context}\\n\\nQuestion: {query}\")\n",
    "            answer = response.content\n",
    "            \n",
    "            # Event: LLM í˜¸ì¶œ ì™„ë£Œ\n",
    "            gen_span.add_event(SpanEvent(\"llm_call_completed\", attributes={\"answer_length\": len(answer)}))\n",
    "            \n",
    "            gen_span.set_outputs({\"answer\": answer})\n",
    "        \n",
    "        # Event: íŒŒì´í”„ë¼ì¸ ì™„ë£Œ\n",
    "        root_span.add_event(SpanEvent(\"pipeline_completed\", attributes={\"success\": True}))\n",
    "        \n",
    "        result = {\"query\": query, \"answer\": answer}\n",
    "        root_span.set_outputs(result)\n",
    "        \n",
    "        return result\n",
    "\n",
    "\n",
    "# Eventsì™€ í•¨ê»˜ ì‹¤í–‰\n",
    "result_with_events = rag_pipeline_with_events(\"Vector StoreëŠ” ë¬´ì—‡ì¸ê°€ìš”?\")\n",
    "\n",
    "print(\"âœ… Eventsê°€ í¬í•¨ëœ Trace ìƒì„± ì™„ë£Œ\")\n",
    "print(f\"ğŸ’¬ ë‹µë³€: {result_with_events['answer'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. MLflow UIì—ì„œ ê²°ê³¼ í™•ì¸\n",
    "\n",
    "### 10.1 UI íƒìƒ‰ ê°€ì´ë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸŒ MLflow UIì—ì„œ ì»¤ìŠ¤í…€ Span í™•ì¸í•˜ê¸°\n",
      "================================================================================\n",
      "\n",
      "1ï¸âƒ£ MLflow UI ì‹¤í–‰:\n",
      "   $ mlflow ui --port 5000\n",
      "\n",
      "2ï¸âƒ£ Experiment ì„ íƒ:\n",
      "   â€¢ 'rag_agent_custom_spans' ì‹¤í—˜ í´ë¦­\n",
      "\n",
      "3ï¸âƒ£ Traces íƒ­:\n",
      "   â€¢ ìƒì„±ëœ Trace ëª©ë¡ í™•ì¸\n",
      "   â€¢ Request ID í´ë¦­í•˜ì—¬ ìƒì„¸ ë³´ê¸°\n",
      "\n",
      "4ï¸âƒ£ Span ê³„ì¸µ êµ¬ì¡°:\n",
      "   â€¢ Waterfall ì°¨íŠ¸ë¡œ ì‹œê°í™”\n",
      "   â€¢ ê° Span í´ë¦­í•˜ì—¬ ìƒì„¸ ì •ë³´ í™•ì¸\n",
      "   â€¢ Inputs, Outputs, Attributes í™•ì¸\n",
      "\n",
      "5ï¸âƒ£ ì„±ëŠ¥ ë¶„ì„:\n",
      "   â€¢ ì‹¤í–‰ ì‹œê°„ì´ ê¸´ Span ì‹ë³„\n",
      "   â€¢ Attributesë¡œ ì„¤ì •ê°’ í™•ì¸\n",
      "   â€¢ Eventsë¡œ ì¤‘ìš” ì´ë²¤íŠ¸ í™•ì¸\n",
      "\n",
      "6ï¸âƒ£ Tags í™œìš©:\n",
      "   â€¢ Trace ë ˆë²¨ íƒœê·¸ë¡œ í•„í„°ë§\n",
      "   â€¢ user_id, session_id ë“±ìœ¼ë¡œ ê²€ìƒ‰\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸŒ MLflow UIì—ì„œ ì»¤ìŠ¤í…€ Span í™•ì¸í•˜ê¸°\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "1ï¸âƒ£ MLflow UI ì‹¤í–‰:\n",
    "   $ mlflow ui --port 5000\n",
    "\n",
    "2ï¸âƒ£ Experiment ì„ íƒ:\n",
    "   â€¢ 'rag_agent_custom_spans' ì‹¤í—˜ í´ë¦­\n",
    "\n",
    "3ï¸âƒ£ Traces íƒ­:\n",
    "   â€¢ ìƒì„±ëœ Trace ëª©ë¡ í™•ì¸\n",
    "   â€¢ Request ID í´ë¦­í•˜ì—¬ ìƒì„¸ ë³´ê¸°\n",
    "\n",
    "4ï¸âƒ£ Span ê³„ì¸µ êµ¬ì¡°:\n",
    "   â€¢ Waterfall ì°¨íŠ¸ë¡œ ì‹œê°í™”\n",
    "   â€¢ ê° Span í´ë¦­í•˜ì—¬ ìƒì„¸ ì •ë³´ í™•ì¸\n",
    "   â€¢ Inputs, Outputs, Attributes í™•ì¸\n",
    "\n",
    "5ï¸âƒ£ ì„±ëŠ¥ ë¶„ì„:\n",
    "   â€¢ ì‹¤í–‰ ì‹œê°„ì´ ê¸´ Span ì‹ë³„\n",
    "   â€¢ Attributesë¡œ ì„¤ì •ê°’ í™•ì¸\n",
    "   â€¢ Eventsë¡œ ì¤‘ìš” ì´ë²¤íŠ¸ í™•ì¸\n",
    "\n",
    "6ï¸âƒ£ Tags í™œìš©:\n",
    "   â€¢ Trace ë ˆë²¨ íƒœê·¸ë¡œ í•„í„°ë§\n",
    "   â€¢ user_id, session_id ë“±ìœ¼ë¡œ ê²€ìƒ‰\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2 ë””ë²„ê¹… ì›Œí¬í”Œë¡œìš°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ› ì»¤ìŠ¤í…€ Spanì„ í™œìš©í•œ ë””ë²„ê¹… ì›Œí¬í”Œë¡œìš°\n",
      "================================================================================\n",
      "\n",
      "ğŸ“‹ ë””ë²„ê¹… ë‹¨ê³„:\n",
      "\n",
      "1. ë¬¸ì œ ë°œê²¬:\n",
      "   â€¢ MLflow UIì—ì„œ ëŠë¦° Trace ì‹ë³„\n",
      "   â€¢ ì‹¤í–‰ ì‹œê°„ì´ ì„ê³„ê°’ì„ ì´ˆê³¼í•˜ëŠ” Trace ì°¾ê¸°\n",
      "\n",
      "2. Span ë¶„ì„:\n",
      "   â€¢ Waterfall ì°¨íŠ¸ì—ì„œ ê°€ì¥ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ëŠ” Span í™•ì¸\n",
      "   â€¢ í•´ë‹¹ Spanì˜ Attributes ê²€í† \n",
      "\n",
      "3. ì›ì¸ íŒŒì•…:\n",
      "   â€¢ Inputs/Outputs í™•ì¸ìœ¼ë¡œ ë°ì´í„° íë¦„ ì¶”ì \n",
      "   â€¢ Attributesì—ì„œ ì„¤ì •ê°’ í™•ì¸ (ì˜ˆ: top_k, model_name)\n",
      "   â€¢ Eventsë¡œ ì¤‘ê°„ ë‹¨ê³„ í™•ì¸\n",
      "\n",
      "4. ê°€ì„¤ ìˆ˜ë¦½:\n",
      "   â€¢ ë³‘ëª© ì§€ì  íŠ¹ì • (ì˜ˆ: retrieval vs generation)\n",
      "   â€¢ ì„¤ì • ë¬¸ì œ vs ë°ì´í„° ë¬¸ì œ êµ¬ë¶„\n",
      "\n",
      "5. ìµœì í™”:\n",
      "   â€¢ íŒŒë¼ë¯¸í„° ì¡°ì • (ì˜ˆ: top_k ê°ì†Œ)\n",
      "   â€¢ ì•Œê³ ë¦¬ì¦˜ ë³€ê²½ (ì˜ˆ: reranking ì¶”ê°€)\n",
      "   â€¢ ìºì‹± ë„ì…\n",
      "\n",
      "6. ê²€ì¦:\n",
      "   â€¢ ìˆ˜ì • í›„ ìƒˆë¡œìš´ Trace ìƒì„±\n",
      "   â€¢ Span ì‹¤í–‰ ì‹œê°„ ë¹„êµ\n",
      "   â€¢ ì„±ëŠ¥ ê°œì„  í™•ì¸\n",
      "\n",
      "ğŸ’¡ Tips:\n",
      "  â€¢ Tagsë¡œ ì‹¤í—˜ ë²„ì „ ê´€ë¦¬\n",
      "  â€¢ Attributesë¡œ A/B í…ŒìŠ¤íŠ¸ ì¶”ì \n",
      "  â€¢ Eventsë¡œ ì¤‘ìš” ì²´í¬í¬ì¸íŠ¸ ê¸°ë¡\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ› ì»¤ìŠ¤í…€ Spanì„ í™œìš©í•œ ë””ë²„ê¹… ì›Œí¬í”Œë¡œìš°\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "ğŸ“‹ ë””ë²„ê¹… ë‹¨ê³„:\n",
    "\n",
    "1. ë¬¸ì œ ë°œê²¬:\n",
    "   â€¢ MLflow UIì—ì„œ ëŠë¦° Trace ì‹ë³„\n",
    "   â€¢ ì‹¤í–‰ ì‹œê°„ì´ ì„ê³„ê°’ì„ ì´ˆê³¼í•˜ëŠ” Trace ì°¾ê¸°\n",
    "\n",
    "2. Span ë¶„ì„:\n",
    "   â€¢ Waterfall ì°¨íŠ¸ì—ì„œ ê°€ì¥ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ëŠ” Span í™•ì¸\n",
    "   â€¢ í•´ë‹¹ Spanì˜ Attributes ê²€í† \n",
    "\n",
    "3. ì›ì¸ íŒŒì•…:\n",
    "   â€¢ Inputs/Outputs í™•ì¸ìœ¼ë¡œ ë°ì´í„° íë¦„ ì¶”ì \n",
    "   â€¢ Attributesì—ì„œ ì„¤ì •ê°’ í™•ì¸ (ì˜ˆ: top_k, model_name)\n",
    "   â€¢ Eventsë¡œ ì¤‘ê°„ ë‹¨ê³„ í™•ì¸\n",
    "\n",
    "4. ê°€ì„¤ ìˆ˜ë¦½:\n",
    "   â€¢ ë³‘ëª© ì§€ì  íŠ¹ì • (ì˜ˆ: retrieval vs generation)\n",
    "   â€¢ ì„¤ì • ë¬¸ì œ vs ë°ì´í„° ë¬¸ì œ êµ¬ë¶„\n",
    "\n",
    "5. ìµœì í™”:\n",
    "   â€¢ íŒŒë¼ë¯¸í„° ì¡°ì • (ì˜ˆ: top_k ê°ì†Œ)\n",
    "   â€¢ ì•Œê³ ë¦¬ì¦˜ ë³€ê²½ (ì˜ˆ: reranking ì¶”ê°€)\n",
    "   â€¢ ìºì‹± ë„ì…\n",
    "\n",
    "6. ê²€ì¦:\n",
    "   â€¢ ìˆ˜ì • í›„ ìƒˆë¡œìš´ Trace ìƒì„±\n",
    "   â€¢ Span ì‹¤í–‰ ì‹œê°„ ë¹„êµ\n",
    "   â€¢ ì„±ëŠ¥ ê°œì„  í™•ì¸\n",
    "\n",
    "ğŸ’¡ Tips:\n",
    "  â€¢ Tagsë¡œ ì‹¤í—˜ ë²„ì „ ê´€ë¦¬\n",
    "  â€¢ Attributesë¡œ A/B í…ŒìŠ¤íŠ¸ ì¶”ì \n",
    "  â€¢ Eventsë¡œ ì¤‘ìš” ì²´í¬í¬ì¸íŠ¸ ê¸°ë¡\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. ìš”ì•½ ë° ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "### âœ… ì™„ë£Œí•œ ì‘ì—…\n",
    "1. ì»¤ìŠ¤í…€ Spanìœ¼ë¡œ RAG íŒŒì´í”„ë¼ì¸ ì„¸ë°€í•˜ê²Œ ì¶”ì \n",
    "2. Span Attributesì™€ Trace Tags í™œìš©\n",
    "3. Span ê³„ì¸µ êµ¬ì¡° ì„¤ê³„ ë° ì‹œê°í™”\n",
    "4. ì„±ëŠ¥ ë³‘ëª© ì§€ì  ì‹ë³„ ë° ë¶„ì„\n",
    "5. Span Eventsë¡œ ì¤‘ìš” ì´ë²¤íŠ¸ ë¡œê¹…\n",
    "6. MLflow UIì—ì„œ ë””ë²„ê¹… ì›Œí¬í”Œë¡œìš° í•™ìŠµ\n",
    "\n",
    "### ğŸ“Š ì£¼ìš” í•™ìŠµ ë‚´ìš©\n",
    "- **`mlflow.start_span()`**: ìˆ˜ë™ Span ìƒì„± ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €\n",
    "- **Span Attributes**: ì‹¤í–‰ ë©”íƒ€ë°ì´í„° (ì„¤ì •ê°’, ëª¨ë¸ëª… ë“±)\n",
    "- **Trace Tags**: ë¹„ì¦ˆë‹ˆìŠ¤ ì»¨í…ìŠ¤íŠ¸ (ì‚¬ìš©ì, ì„¸ì…˜, í™˜ê²½)\n",
    "- **SpanType**: RETRIEVER, LLM, CHAIN ë“± ì‚¬ì „ ì •ì˜ íƒ€ì…\n",
    "- **ê³„ì¸µ êµ¬ì¡°**: Parent-Child ê´€ê³„ë¡œ ë³µì¡í•œ íŒŒì´í”„ë¼ì¸ í‘œí˜„\n",
    "- **ì„±ëŠ¥ ë¶„ì„**: Spanë³„ ì‹¤í–‰ ì‹œê°„ìœ¼ë¡œ ë³‘ëª© ì§€ì  ì‹ë³„\n",
    "\n",
    "### ğŸ¯ ë‹¤ìŒ ë‹¨ê³„ (Step 4)\n",
    "ë‹¤ìŒ ë…¸íŠ¸ë¶ì—ì„œëŠ” **MLflowì˜ Built-in Scorers**ë¥¼ í™œìš©í•˜ì—¬:\n",
    "- Faithfulness, Relevance, Correctness í‰ê°€\n",
    "- LLM-as-Judge ë©”íŠ¸ë¦­ ì‚¬ìš©\n",
    "- RAG í’ˆì§ˆ ìë™ ì¸¡ì •\n",
    "- í‰ê°€ ê²°ê³¼ ì¶”ì  ë° ë¹„êµ\n",
    "\n",
    "â†’ `04_mlflow_evaluation_builtin_scorers.ipynb`ë¡œ ê³„ì†í•˜ì„¸ìš”!\n",
    "\n",
    "### ğŸ’¡ ì»¤ìŠ¤í…€ Spanì˜ ì¥ì \n",
    "1. **ì„¸ë°€í•œ ì¶”ì **: íŒŒì´í”„ë¼ì¸ì˜ ê° ë‹¨ê³„ë¥¼ ëª…í™•íˆ êµ¬ë¶„\n",
    "2. **ì„±ëŠ¥ ìµœì í™”**: ë³‘ëª© ì§€ì ì„ ì •í™•íˆ ì‹ë³„\n",
    "3. **ë””ë²„ê¹…**: ë¬¸ì œ ë°œìƒ ìœ„ì¹˜ë¥¼ ë¹ ë¥´ê²Œ íŒŒì•…\n",
    "4. **ë¹„ì¦ˆë‹ˆìŠ¤ ì»¨í…ìŠ¤íŠ¸**: Tagsë¡œ ì‚¬ìš©ì/ì„¸ì…˜ ì¶”ì \n",
    "5. **ìœ ì—°ì„±**: Autologë¡œ ë¶ˆê°€ëŠ¥í•œ ì»¤ìŠ¤í…€ ë¡œì§ ì¶”ì "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“š ì°¸ê³  ìë£Œ\n",
    "\n",
    "- [MLflow Tracing - Manual Instrumentation](https://mlflow.org/docs/latest/tracing/api/manual-instrumentation)\n",
    "- [Span Concepts](https://mlflow.org/docs/latest/genai/concepts/span/)\n",
    "- [SpanType Documentation](https://mlflow.org/docs/latest/python_api/mlflow.entities.html#mlflow.entities.SpanType)\n",
    "- [Tracing Best Practices](https://mlflow.org/docs/latest/genai/tracing/)\n",
    "- [OpenTelemetry Specification](https://opentelemetry.io/docs/specs/otel/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlflow-genai-tutorial (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
